# üåä Criando Pipelines ETL com Apache Airflow

## üìö Conceitos B√°sicos

### O que √© um DAG?
DAG (Directed Acyclic Graph) √© um pipeline de tarefas no Airflow. Cada tarefa representa uma opera√ß√£o (extra√ß√£o, transforma√ß√£o, carga, etc).

### Estrutura de um DAG
```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

# Configura√ß√µes padr√£o
default_args = {
    'owner': 'data-team',
    'retries': 2,
    'retry_delay': timedelta(minutes=5)
}

# Defini√ß√£o do DAG
with DAG(
    'meu_pipeline_etl',
    default_args=default_args,
    description='Pipeline ETL simples',
    schedule_interval='@daily',
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['etl', 'exemplo']
) as dag:
    
    # Tarefas aqui
    pass
```

## üéØ Exemplo 1: Pipeline ETL B√°sico

### Arquivo: `dags/etl_vendas_basico.py`

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import pandas as pd
import boto3
from io import StringIO

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Configura√ß√£o MinIO (S3)
s3_config = {
    'endpoint_url': 'http://minio:9000',
    'aws_access_key_id': 'minioadmin',
    'aws_secret_access_key': 'minioadmin123'
}

def extrair_dados_bronze(**context):
    """Extrai dados do bucket bronze"""
    s3 = boto3.client('s3', **s3_config)
    
    # Ler arquivo CSV do bronze
    obj = s3.get_object(Bucket='bronze', Key='vendas/vendas.csv')
    df = pd.read_csv(obj['Body'])
    
    print(f"‚úÖ Extra√≠dos {len(df)} registros")
    
    # Salvar em XCom para pr√≥xima tarefa
    context['task_instance'].xcom_push(key='dados_brutos', value=df.to_json())
    return len(df)

def transformar_dados(**context):
    """Transforma e limpa os dados"""
    # Recuperar dados da tarefa anterior
    dados_json = context['task_instance'].xcom_pull(
        task_ids='extrair_bronze',
        key='dados_brutos'
    )
    df = pd.read_json(dados_json)
    
    # Transforma√ß√µes
    df['data'] = pd.to_datetime(df['data'])
    df['valor_total'] = df['quantidade'] * df['valor']
    df['mes'] = df['data'].dt.to_period('M').astype(str)
    
    # Valida√ß√µes
    df = df[df['quantidade'] > 0]
    df = df[df['valor'] > 0]
    df = df.dropna()
    
    print(f"‚úÖ Transformados {len(df)} registros v√°lidos")
    
    context['task_instance'].xcom_push(key='dados_limpos', value=df.to_json())
    return len(df)

def carregar_dados_silver(**context):
    """Carrega dados no bucket silver"""
    # Recuperar dados transformados
    dados_json = context['task_instance'].xcom_pull(
        task_ids='transformar',
        key='dados_limpos'
    )
    df = pd.read_json(dados_json)
    
    # Salvar no MinIO (silver)
    s3 = boto3.client('s3', **s3_config)
    
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, index=False)
    
    s3.put_object(
        Bucket='silver',
        Key='vendas/vendas_processadas.csv',
        Body=csv_buffer.getvalue()
    )
    
    print(f"‚úÖ Carregados {len(df)} registros no silver")
    return len(df)

def agregar_dados(**context):
    """Agrega dados para an√°lise"""
    dados_json = context['task_instance'].xcom_pull(
        task_ids='transformar',
        key='dados_limpos'
    )
    df = pd.read_json(dados_json)
    
    # Agrega√ß√µes
    vendas_por_mes = df.groupby('mes').agg({
        'quantidade': 'sum',
        'valor_total': 'sum'
    }).reset_index()
    
    vendas_por_produto = df.groupby('produto').agg({
        'quantidade': 'sum',
        'valor_total': 'sum'
    }).reset_index()
    
    # Salvar agrega√ß√µes no gold
    s3 = boto3.client('s3', **s3_config)
    
    # Vendas por m√™s
    csv_buffer = StringIO()
    vendas_por_mes.to_csv(csv_buffer, index=False)
    s3.put_object(
        Bucket='gold',
        Key='vendas/vendas_por_mes.csv',
        Body=csv_buffer.getvalue()
    )
    
    # Vendas por produto
    csv_buffer = StringIO()
    vendas_por_produto.to_csv(csv_buffer, index=False)
    s3.put_object(
        Bucket='gold',
        Key='vendas/vendas_por_produto.csv',
        Body=csv_buffer.getvalue()
    )
    
    print(f"‚úÖ Agrega√ß√µes salvas no gold")

# Defini√ß√£o do DAG
with DAG(
    'etl_vendas_basico',
    default_args=default_args,
    description='Pipeline ETL de vendas - Bronze ‚Üí Silver ‚Üí Gold',
    schedule_interval='@daily',
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['etl', 'vendas', 'exemplo']
) as dag:
    
    # Definir tarefas
    extrair = PythonOperator(
        task_id='extrair_bronze',
        python_callable=extrair_dados_bronze
    )
    
    transformar = PythonOperator(
        task_id='transformar',
        python_callable=transformar_dados
    )
    
    carregar = PythonOperator(
        task_id='carregar_silver',
        python_callable=carregar_dados_silver
    )
    
    agregar = PythonOperator(
        task_id='agregar_gold',
        python_callable=agregar_dados
    )
    
    # Definir ordem de execu√ß√£o
    extrair >> transformar >> [carregar, agregar]
```

## üìÇ Onde Colocar o DAG

```bash
# Copiar para a pasta de DAGs
cp etl_vendas_basico.py /media/marcelo/dados1/bigdata-docker/airflow/dags/

# Ou usar o volume montado
cp etl_vendas_basico.py ./examples/dags/
```

O Airflow detecta novos DAGs automaticamente em cerca de 30 segundos.

## ‚ñ∂Ô∏è Executando o Pipeline

### Via Interface Web

1. Acesse http://localhost:8080
2. Fa√ßa login (airflow/airflow)
3. Localize o DAG `etl_vendas_basico`
4. Ative o toggle para habilitar
5. Clique em "‚ñ∂" (Trigger DAG) para executar

### Via CLI

```bash
# Entrar no container
docker exec -it airflow-webserver bash

# Listar DAGs
airflow dags list

# Testar uma tarefa espec√≠fica
airflow tasks test etl_vendas_basico extrair_bronze 2025-01-01

# Executar o DAG completo
airflow dags trigger etl_vendas_basico
```

## üîç Monitorando a Execu√ß√£o

### Ver Status
- **Graph View:** Visualiza√ß√£o gr√°fica das tarefas
- **Tree View:** Hist√≥rico de execu√ß√µes
- **Gantt View:** Timeline de execu√ß√£o
- **Logs:** Clique em uma tarefa ‚Üí View Log

### Verificar Dados Gerados

```bash
# Listar arquivos no MinIO
docker exec -it minio-client mc ls myminio/silver/vendas/
docker exec -it minio-client mc ls myminio/gold/vendas/
```

## üéØ Exemplo 2: Pipeline com Spark

### Arquivo: `dags/etl_vendas_spark.py`

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data-team',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'etl_vendas_spark',
    default_args=default_args,
    description='Pipeline ETL usando Spark',
    schedule_interval='@daily',
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['etl', 'spark', 'vendas']
) as dag:
    
    processar_spark = SparkSubmitOperator(
        task_id='processar_vendas_spark',
        application='/opt/airflow/jobs/process_sales.py',
        conn_id='spark_default',
        conf={
            'spark.master': 'spark://spark-master:7077',
            'spark.executor.memory': '2g',
            'spark.executor.cores': '2'
        },
        application_args=[
            '--input-path', 's3a://bronze/vendas/',
            '--output-path', 's3a://silver/vendas/'
        ]
    )
```

## üìä Boas Pr√°ticas

### 1. Usar XCom para Passar Dados entre Tarefas
```python
# Enviar dados
context['task_instance'].xcom_push(key='minha_chave', value=dados)

# Receber dados
dados = context['task_instance'].xcom_pull(task_ids='tarefa_anterior', key='minha_chave')
```

### 2. Tratamento de Erros
```python
def minha_tarefa(**context):
    try:
        # Seu c√≥digo
        pass
    except Exception as e:
        print(f"‚ùå Erro: {e}")
        raise  # Re-lan√ßar para marcar tarefa como falha
```

### 3. Logging Adequado
```python
import logging

def minha_tarefa(**context):
    logging.info("Iniciando processamento...")
    logging.warning("Aten√ß√£o: dados inconsistentes")
    logging.error("Erro cr√≠tico!")
```

### 4. Parametriza√ß√£o
```python
from airflow.models import Variable

# Definir vari√°vel na UI: Admin ‚Üí Variables
db_conn = Variable.get("database_connection")
```

## üîÑ Agendamento de DAGs

```python
# Diariamente √†s 2h da manh√£
schedule_interval='0 2 * * *'

# A cada hora
schedule_interval='@hourly'

# Semanalmente (segunda-feira)
schedule_interval='0 0 * * 1'

# Apenas manual
schedule_interval=None
```

## üìã Depend√™ncias Externas

### Instalar Pacotes Python no Airflow

1. Criar arquivo `requirements.txt`:
```
pandas==2.0.0
boto3==1.26.0
sqlalchemy==2.0.0
```

2. Adicionar ao docker-compose.yml:
```yaml
environment:
  _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:- pandas boto3 sqlalchemy}
```

## üÜò Troubleshooting

### DAG n√£o aparece
- Verifique erros de sintaxe Python
- Aguarde 30 segundos (intervalo de scan)
- Verifique logs: `docker compose logs airflow-scheduler`

### Tarefa falha
- Clique na tarefa ‚Üí View Log
- Verifique vari√°veis de ambiente
- Teste a fun√ß√£o Python isoladamente

### Dados n√£o aparecem no MinIO
- Verifique credenciais S3
- Confirme permiss√µes do bucket
- Verifique logs da tarefa

## üéì Pr√≥ximos Passos

- **Processamento com Spark:** `03-processamento-spark.md`
- **Consultas SQL:** `04-consultas-trino.md`
- **Dashboards:** `05-criando-dashboards-superset.md`
