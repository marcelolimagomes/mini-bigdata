# ğŸ“š DocumentaÃ§Ã£o - Mini Big Data Stack

## ğŸ¯ Bem-vindo!

Esta Ã© a documentaÃ§Ã£o completa da stack Big Data enxuta para desenvolvimento de pipelines ETL e dashboards BI.

## ğŸ“– Guias DisponÃ­veis

### ğŸš€ [01 - Guia de InÃ­cio RÃ¡pido](./01-guia-inicio-rapido.md)
Comece aqui! Aprenda sobre a arquitetura, acesse os serviÃ§os e crie seu primeiro projeto.

**VocÃª vai aprender:**
- VisÃ£o geral da arquitetura (MinIO, Spark, Trino, Airflow, Superset)
- URLs de acesso e credenciais
- Conceito de camadas Bronze â†’ Silver â†’ Gold
- Criar seu primeiro pipeline ETL simples

**Tempo:** 30 minutos

---

### ğŸ”„ [02 - Criando Pipelines com Airflow](./02-criando-pipelines-airflow.md)
Domine a orquestraÃ§Ã£o de pipelines ETL com Apache Airflow.

**VocÃª vai aprender:**
- Estrutura de DAGs (Directed Acyclic Graphs)
- Criar pipeline Bronze â†’ Silver â†’ Gold completo
- Integrar Airflow com Spark
- Agendar execuÃ§Ãµes e monitorar pipelines
- Usar XCom para passar dados entre tasks

**Tempo:** 1-2 horas

---

### âš¡ [03 - Processamento com Spark](./03-processamento-spark.md)
Desenvolva jobs de processamento distribuÃ­do com Apache Spark.

**VocÃª vai aprender:**
- Escrever jobs PySpark
- Conectar Spark ao MinIO (S3)
- TransformaÃ§Ãµes e agregaÃ§Ãµes
- ValidaÃ§Ã£o de qualidade de dados
- Executar jobs via spark-submit, Airflow ou shell

**Tempo:** 1-2 horas

---

### ğŸ” [04 - Consultas SQL com Trino](./04-consultas-trino.md)
Execute queries SQL distribuÃ­das sobre o Data Lake.

**VocÃª vai aprender:**
- Conectar ao Trino (CLI, Python, SQLAlchemy)
- Criar schemas e tabelas
- Queries SQL avanÃ§adas (window functions, CTEs, joins)
- OtimizaÃ§Ã£o de performance
- AnÃ¡lises analÃ­ticas complexas

**Tempo:** 1 hora

---

### ğŸ“Š [05 - Criando Dashboards com Superset](./05-criando-dashboards-superset.md)
Construa visualizaÃ§Ãµes e dashboards interativos de BI.

**VocÃª vai aprender:**
- Conectar Superset ao Trino
- Criar diversos tipos de grÃ¡ficos (linha, barra, pizza, KPIs, heatmap)
- Montar dashboards completos com filtros
- SQL customizado e datasets virtuais
- Agendar envio de relatÃ³rios

**Tempo:** 1-2 horas

---

### ğŸ—„ï¸ [06 - CatÃ¡logo Hive Metastore](./06-catalogo-hive-metastore.md)
Gerencie o catÃ¡logo de metadados central do Data Lake.

**VocÃª vai aprender:**
- Criar databases e tabelas
- Tabelas gerenciadas vs externas
- Particionamento estratÃ©gico
- Schema evolution
- Formatos de arquivo (Parquet, ORC, Avro)
- Consultar metadados internos

**Tempo:** 1 hora

---

### ğŸ”Œ [07 - APIs REST e JDBC](./07-apis-rest-jdbc.md)
Acesse dados programaticamente via APIs e JDBC.

**VocÃª vai aprender:**
- Conectar via JDBC (Java, Python, R)
- Usar Trino REST API
- Integrar com Airflow REST API
- Acessar Superset API
- Usar MinIO S3 API (boto3)
- Criar APIs customizadas (Flask, Express)

**Tempo:** 1-2 horas

---

## ğŸ“ Trilhas de Aprendizado

### ğŸ‘¨â€ğŸ’» Desenvolvedor Iniciante
1. **InÃ­cio RÃ¡pido** â†’ Entenda a arquitetura
2. **Consultas Trino** â†’ Execute SQL bÃ¡sico
3. **Dashboards Superset** â†’ Visualize dados existentes

**Tempo total:** ~3 horas

---

### ğŸ§‘â€ğŸ”§ Engenheiro de Dados
1. **InÃ­cio RÃ¡pido** â†’ VisÃ£o geral
2. **Pipelines Airflow** â†’ OrquestraÃ§Ã£o ETL
3. **Processamento Spark** â†’ TransformaÃ§Ãµes
4. **CatÃ¡logo Hive** â†’ GestÃ£o de metadados
5. **APIs REST/JDBC** â†’ IntegraÃ§Ã£o programÃ¡tica

**Tempo total:** 6-8 horas

---

### ğŸ“Š Analista de BI
1. **InÃ­cio RÃ¡pido** â†’ Arquitetura bÃ¡sica
2. **Consultas Trino** â†’ SQL avanÃ§ado
3. **Dashboards Superset** â†’ VisualizaÃ§Ãµes
4. **APIs REST/JDBC** â†’ AutomatizaÃ§Ã£o

**Tempo total:** 4-5 horas

---

## ğŸ—ï¸ Arquitetura da Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Camada BI                        â”‚
â”‚  ğŸ“Š Apache Superset (Dashboards e VisualizaÃ§Ãµes)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Camada de Query                      â”‚
â”‚      ğŸ” Trino (SQL Engine DistribuÃ­do)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Camada de CatÃ¡logo                    â”‚
â”‚    ğŸ—„ï¸ Hive Metastore (Schemas, Tabelas, PartiÃ§Ãµes) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Camada de Storage                      â”‚
â”‚         ğŸª£ MinIO (Object Storage S3)                â”‚
â”‚    [bronze/] [silver/] [gold/] [warehouse/]        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OrquestraÃ§Ã£o ETL â”‚  â”‚  Processamento   â”‚
â”‚ ğŸ”„ Apache Airflowâ”‚  â”‚ âš¡ Apache Spark  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸŒ URLs de Acesso RÃ¡pido

| ServiÃ§o | URL | UsuÃ¡rio | Senha |
|---------|-----|---------|-------|
| **MinIO** | http://localhost:9001 | `minioadmin` | `minioadmin` |
| **Airflow** | http://localhost:8080 | `airflow` | `airflow` |
| **Trino** | http://localhost:8081 | `trino` | - |
| **Superset** | http://localhost:8088 | `admin` | `admin` |
| **Spark Master** | http://localhost:8082 | - | - |

## ğŸ“‚ Estrutura de Pastas

```
mini-bigdata/
â”œâ”€â”€ docker-compose.yml          # OrquestraÃ§Ã£o dos containers
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ hive/                   # ConfiguraÃ§Ã£o Hive Metastore
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ superset/               # Scripts Superset
â”‚   â”‚   â””â”€â”€ init-superset.sh
â”‚   â””â”€â”€ airflow/
â”‚       â””â”€â”€ dags/               # DAGs do Airflow
â”œâ”€â”€ docs/                       # ğŸ“š VOCÃŠ ESTÃ AQUI!
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ 01-guia-inicio-rapido.md
â”‚   â”œâ”€â”€ 02-criando-pipelines-airflow.md
â”‚   â”œâ”€â”€ 03-processamento-spark.md
â”‚   â”œâ”€â”€ 04-consultas-trino.md
â”‚   â”œâ”€â”€ 05-criando-dashboards-superset.md
â”‚   â”œâ”€â”€ 06-catalogo-hive-metastore.md
â”‚   â”œâ”€â”€ 07-apis-rest-jdbc.md
â”‚   â””â”€â”€ senhas.txt
â””â”€â”€ data/                       # Dados de exemplo (opcional)
```

## ğŸš€ Quick Start

```bash
# 1. Subir a stack completa
docker compose up -d

# 2. Verificar serviÃ§os
docker compose ps

# 3. Acessar Airflow
# http://localhost:8080 (airflow/airflow)

# 4. Acessar Superset
# http://localhost:8088 (admin/admin)

# 5. Ver logs (se necessÃ¡rio)
docker compose logs -f <serviÃ§o>
```

## ğŸ”§ Comandos Ãšteis

### Docker
```bash
# Parar todos os serviÃ§os
docker compose down

# Recriar um serviÃ§o especÃ­fico
docker compose up -d --force-recreate <serviÃ§o>

# Ver logs de um serviÃ§o
docker compose logs -f <serviÃ§o>

# Executar comando em container
docker compose exec <serviÃ§o> bash
```

### Spark
```bash
# Abrir Spark Shell
docker compose exec spark-master spark-shell

# PySpark
docker compose exec spark-master pyspark

# Submeter job
docker compose exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  /path/to/job.py
```

### Trino
```bash
# Trino CLI
docker compose exec trino trino --catalog hive --schema default
```

### PostgreSQL
```bash
# Acessar banco Hive Metastore
docker compose exec postgres psql -U postgres -d hive_metastore

# Acessar banco Airflow
docker compose exec postgres psql -U postgres -d airflow_db

# Acessar banco Superset
docker compose exec postgres psql -U postgres -d superset_db
```

## ğŸ†˜ Troubleshooting

### ServiÃ§o nÃ£o sobe
```bash
# Ver logs detalhados
docker compose logs <serviÃ§o>

# Recriar volume e container
docker compose down
docker compose up -d --force-recreate <serviÃ§o>
```

### Falta de espaÃ§o
```bash
# Verificar espaÃ§o em disco
df -h /media/marcelo/dados1/bigdata-docker/

# Limpar dados antigos (cuidado!)
docker system prune -a --volumes
```

### Erro de conexÃ£o entre serviÃ§os
```bash
# Verificar rede Docker
docker network inspect mini-bigdata_default

# Reiniciar stack completa
docker compose down
docker compose up -d
```

### Airflow: DAG nÃ£o aparece
```bash
# Verificar logs do scheduler
docker compose logs airflow-scheduler

# Restartar scheduler
docker compose restart airflow-scheduler
```

## ğŸ“Š Exemplo Completo: Pipeline ETL â†’ BI

### 1. Dados Bronze (Raw)
```python
# Ingerir CSV para MinIO bronze/
import pandas as pd
df = pd.read_csv('vendas.csv')
df.to_csv('s3://bronze/vendas/raw.csv')
```

### 2. Processar â†’ Silver (Airflow + Spark)
```python
# DAG Airflow executando job Spark
# Limpa dados, valida, converte para Parquet
```

### 3. Agregar â†’ Gold (Spark)
```python
# AgregaÃ§Ãµes e mÃ©tricas de negÃ³cio
# Tabelas prontas para BI
```

### 4. Catalogar (Hive Metastore)
```sql
CREATE EXTERNAL TABLE vendas.vendas_gold (...)
LOCATION 's3a://gold/vendas/';
```

### 5. Consultar (Trino)
```sql
SELECT produto, SUM(receita) 
FROM hive.vendas.vendas_gold
GROUP BY produto;
```

### 6. Visualizar (Superset)
```
Dashboard com grÃ¡ficos de receita, tendÃªncias, KPIs
```

## ğŸ¯ Boas PrÃ¡ticas

### ğŸ“ OrganizaÃ§Ã£o de Dados
- **Bronze:** Dados raw, formato original
- **Silver:** Dados limpos, validados, Parquet
- **Gold:** Dados agregados, prontos para BI

### ğŸ”„ ETL
- Use Airflow para orquestraÃ§Ã£o
- Spark para processamento pesado
- Valide qualidade dos dados
- Registre no Hive Metastore

### ğŸ“Š BI
- Crie datasets no Superset a partir do Trino
- Use SQL Lab para queries ad-hoc
- Dashboards com filtros interativos

### ğŸ›¡ï¸ SeguranÃ§a
- Mude senhas padrÃ£o em produÃ§Ã£o
- Use autenticaÃ§Ã£o (OAuth, LDAP)
- Restrinja acesso via firewall

## ğŸ“ Suporte

### DocumentaÃ§Ã£o Oficial
- [Apache Airflow](https://airflow.apache.org/docs/)
- [Apache Spark](https://spark.apache.org/docs/latest/)
- [Trino](https://trino.io/docs/current/)
- [Apache Superset](https://superset.apache.org/docs/intro)
- [MinIO](https://min.io/docs/minio/linux/index.html)
- [Hive](https://cwiki.apache.org/confluence/display/Hive/Home)

### Issues Conhecidos
Veja `senhas.txt` para troubleshooting de autenticaÃ§Ã£o.

## ğŸ“ PrÃ³ximos Passos

1. âœ… Leia o **Guia de InÃ­cio RÃ¡pido**
2. âœ… Crie seu primeiro pipeline no **Airflow**
3. âœ… Processe dados com **Spark**
4. âœ… Consulte via **Trino**
5. âœ… Construa dashboards no **Superset**

**Boa jornada no mundo Big Data! ğŸš€**
