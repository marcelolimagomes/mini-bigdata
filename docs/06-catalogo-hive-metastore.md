# ðŸ—„ï¸ CatÃ¡logo Hive Metastore

## ðŸŽ¯ VisÃ£o Geral

O Hive Metastore Ã© o catÃ¡logo central de metadados para todas as tabelas e schemas do Data Lake.

- **Porta:** 9083
- **Database Backend:** PostgreSQL
- **Compatibilidade:** Spark, Trino, Hive

## ðŸ” Arquitetura do Metastore

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Clientes (Spark/Trino)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ Thrift (9083)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Hive Metastore Service        â”‚
â”‚   - Gerencia schemas e tabelas      â”‚
â”‚   - Armazena metadados              â”‚
â”‚   - Registra partiÃ§Ãµes              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ JDBC
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         PostgreSQL Backend          â”‚
â”‚   - Tabela: DBS (databases)         â”‚
â”‚   - Tabela: TBLS (tables)           â”‚
â”‚   - Tabela: COLUMNS_V2              â”‚
â”‚   - Tabela: PARTITIONS              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸš€ Gerenciamento via Spark

### Conectar ao Spark Shell

```bash
docker compose exec spark-master spark-shell
```

### Criar Database

```scala
// Criar database
spark.sql("CREATE DATABASE IF NOT EXISTS vendas")

// Com localizaÃ§Ã£o customizada
spark.sql("""
  CREATE DATABASE IF NOT EXISTS analytics
  LOCATION 's3a://gold/analytics/'
  COMMENT 'Database para analytics'
""")

// Listar databases
spark.sql("SHOW DATABASES").show()
```

### Criar Tabelas

#### Tabela Gerenciada

```scala
// Tabela gerenciada (dados no warehouse do Hive)
spark.sql("""
  CREATE TABLE vendas.produtos (
    id INT,
    nome STRING,
    categoria STRING,
    preco DECIMAL(10,2),
    data_criacao TIMESTAMP
  )
  USING PARQUET
  COMMENT 'CatÃ¡logo de produtos'
""")
```

#### Tabela Externa

```scala
// Tabela externa (dados no MinIO)
spark.sql("""
  CREATE EXTERNAL TABLE vendas.vendas_raw (
    id STRING,
    data STRING,
    produto STRING,
    quantidade INT,
    valor DOUBLE
  )
  USING CSV
  OPTIONS (
    path 's3a://bronze/vendas/raw/',
    header 'true',
    delimiter ','
  )
""")
```

#### Tabela Particionada

```scala
// Tabela particionada por ano e mÃªs
spark.sql("""
  CREATE TABLE vendas.vendas_particionada (
    id STRING,
    data DATE,
    produto STRING,
    quantidade INT,
    valor DECIMAL(10,2)
  )
  USING PARQUET
  PARTITIONED BY (ano INT, mes INT)
  LOCATION 's3a://silver/vendas/particionada/'
""")

// Inserir dados com partiÃ§Ã£o
spark.sql("""
  INSERT INTO vendas.vendas_particionada
  PARTITION (ano=2025, mes=1)
  VALUES 
    ('V001', '2025-01-15', 'Notebook', 2, 4500.00),
    ('V002', '2025-01-20', 'Mouse', 5, 150.00)
""")
```

### Gerenciar PartiÃ§Ãµes

```scala
// Listar partiÃ§Ãµes
spark.sql("SHOW PARTITIONS vendas.vendas_particionada").show()

// Adicionar partiÃ§Ã£o manualmente
spark.sql("""
  ALTER TABLE vendas.vendas_particionada
  ADD PARTITION (ano=2025, mes=2)
  LOCATION 's3a://silver/vendas/particionada/ano=2025/mes=2/'
""")

// Remover partiÃ§Ã£o
spark.sql("""
  ALTER TABLE vendas.vendas_particionada
  DROP PARTITION (ano=2025, mes=2)
""")

// Reparar partiÃ§Ãµes (descobrir automaticamente)
spark.sql("MSCK REPAIR TABLE vendas.vendas_particionada")
```

### Propriedades de Tabelas

```scala
// Adicionar propriedades
spark.sql("""
  ALTER TABLE vendas.produtos
  SET TBLPROPERTIES (
    'description' = 'Tabela de produtos atualizada',
    'last_modified_by' = 'analytics_team',
    'version' = '2.0'
  )
""")

// Ver propriedades
spark.sql("SHOW TBLPROPERTIES vendas.produtos").show()
```

### EstatÃ­sticas

```scala
// Coletar estatÃ­sticas da tabela
spark.sql("ANALYZE TABLE vendas.produtos COMPUTE STATISTICS")

// EstatÃ­sticas de colunas
spark.sql("""
  ANALYZE TABLE vendas.produtos 
  COMPUTE STATISTICS FOR COLUMNS id, preco
""")

// Ver estatÃ­sticas
spark.sql("DESCRIBE EXTENDED vendas.produtos").show(false)
```

## ðŸ”§ Gerenciamento via Trino

### Conectar ao Trino CLI

```bash
docker compose exec trino trino --catalog hive --schema default
```

### Gerenciar Schemas

```sql
-- Criar schema
CREATE SCHEMA IF NOT EXISTS hive.vendas
WITH (location = 's3a://gold/vendas/');

-- Listar schemas
SHOW SCHEMAS IN hive;

-- Ver detalhes
SHOW CREATE SCHEMA hive.vendas;

-- Remover schema
DROP SCHEMA IF EXISTS hive.analytics;
```

### Gerenciar Tabelas

```sql
-- Criar tabela ORC
CREATE TABLE hive.vendas.clientes (
    id BIGINT,
    nome VARCHAR,
    email VARCHAR,
    telefone VARCHAR,
    data_cadastro TIMESTAMP
)
WITH (
    format = 'ORC',
    external_location = 's3a://gold/vendas/clientes/'
);

-- Criar tabela a partir de query
CREATE TABLE hive.vendas.vendas_agregadas AS
SELECT 
    produto,
    YEAR(CAST(data AS DATE)) AS ano,
    MONTH(CAST(data AS DATE)) AS mes,
    SUM(quantidade) AS quantidade_total,
    SUM(valor_total) AS receita_total
FROM hive.vendas.vendas_silver
GROUP BY produto, YEAR(CAST(data AS DATE)), MONTH(CAST(data AS DATE));

-- Listar tabelas
SHOW TABLES IN hive.vendas;

-- Ver estrutura
DESCRIBE hive.vendas.clientes;

-- Ver DDL completo
SHOW CREATE TABLE hive.vendas.clientes;
```

### Modificar Tabelas

```sql
-- Adicionar coluna
ALTER TABLE hive.vendas.clientes 
ADD COLUMN cidade VARCHAR;

-- Renomear tabela
ALTER TABLE hive.vendas.clientes 
RENAME TO hive.vendas.cadastro_clientes;

-- Remover tabela
DROP TABLE IF EXISTS hive.vendas.temp_data;
```

## ðŸ“Š Formatos de Arquivo Suportados

### Parquet (Recomendado)

```scala
// Criar tabela Parquet
spark.sql("""
  CREATE TABLE vendas.dados_parquet (
    id INT,
    valor DECIMAL(10,2),
    data DATE
  )
  USING PARQUET
  OPTIONS (
    compression 'snappy'
  )
""")
```

**Vantagens:**
- CompressÃ£o eficiente
- Schema embarcado
- Otimizado para queries analÃ­ticas
- Suporta nested data

### ORC

```sql
-- Trino: Criar tabela ORC
CREATE TABLE hive.vendas.dados_orc (
    id BIGINT,
    valor DECIMAL(10,2),
    data DATE
)
WITH (
    format = 'ORC',
    orc_compression = 'ZLIB'
);
```

**Vantagens:**
- Alta compressÃ£o
- Ãndices integrados
- Ideal para Hive

### Avro

```scala
// Spark: Tabela Avro
spark.sql("""
  CREATE TABLE vendas.dados_avro (
    id INT,
    dados STRING
  )
  USING AVRO
  LOCATION 's3a://bronze/avro/'
""")
```

**Vantagens:**
- Schema evolution
- Bom para streaming
- Interoperabilidade

### CSV (Ingest)

```scala
// Apenas para ingestÃ£o inicial
spark.sql("""
  CREATE EXTERNAL TABLE vendas.raw_csv (
    col1 STRING,
    col2 STRING,
    col3 STRING
  )
  USING CSV
  OPTIONS (
    path 's3a://bronze/raw/',
    header 'true',
    inferSchema 'false'
  )
""")
```

## ðŸ” Particionamento EstratÃ©gico

### Particionamento por Data

```scala
// Particionamento diÃ¡rio
spark.sql("""
  CREATE TABLE vendas.log_eventos (
    evento_id STRING,
    usuario STRING,
    acao STRING,
    timestamp TIMESTAMP
  )
  PARTITIONED BY (dt DATE)
  STORED AS PARQUET
""")

// Inserir com partiÃ§Ã£o automÃ¡tica
spark.sql("""
  INSERT INTO vendas.log_eventos
  PARTITION (dt)
  SELECT 
    evento_id,
    usuario,
    acao,
    timestamp,
    CAST(timestamp AS DATE) as dt
  FROM vendas.eventos_raw
""")
```

### Particionamento HierÃ¡rquico

```scala
// Ano â†’ MÃªs â†’ Dia
spark.sql("""
  CREATE TABLE vendas.transacoes (
    id STRING,
    valor DECIMAL(10,2),
    tipo STRING
  )
  PARTITIONED BY (ano INT, mes INT, dia INT)
  STORED AS PARQUET
""")
```

### Bucketing (Clustering)

```scala
// Bucketing para joins eficientes
spark.sql("""
  CREATE TABLE vendas.vendas_bucketed (
    venda_id STRING,
    cliente_id STRING,
    valor DECIMAL(10,2),
    data DATE
  )
  CLUSTERED BY (cliente_id) INTO 50 BUCKETS
  STORED AS PARQUET
""")
```

## ðŸ” Consultar Metadados Internos

### Consultar PostgreSQL Diretamente

```bash
# Conectar ao PostgreSQL
docker compose exec postgres psql -U postgres -d hive_metastore
```

```sql
-- Listar todos os databases
SELECT * FROM "DBS";

-- Listar todas as tabelas
SELECT 
    d."NAME" as database_name,
    t."TBL_NAME" as table_name,
    t."TBL_TYPE" as table_type,
    s."LOCATION" as location
FROM "TBLS" t
JOIN "DBS" d ON t."DB_ID" = d."DB_ID"
JOIN "SDS" s ON t."SD_ID" = s."SD_ID";

-- Ver colunas de uma tabela
SELECT 
    c."COLUMN_NAME",
    c."TYPE_NAME",
    c."INTEGER_IDX" as position
FROM "COLUMNS_V2" c
JOIN "SDS" s ON c."CD_ID" = s."CD_ID"
JOIN "TBLS" t ON t."SD_ID" = s."SD_ID"
WHERE t."TBL_NAME" = 'vendas_silver'
ORDER BY c."INTEGER_IDX";

-- Ver partiÃ§Ãµes
SELECT 
    p."PART_NAME",
    s."LOCATION"
FROM "PARTITIONS" p
JOIN "SDS" s ON p."SD_ID" = s."SD_ID"
JOIN "TBLS" t ON p."TBL_ID" = t."TBL_ID"
WHERE t."TBL_NAME" = 'vendas_particionada';
```

## ðŸ› ï¸ ManutenÃ§Ã£o do CatÃ¡logo

### Backup do Metastore

```bash
# Backup do PostgreSQL
docker compose exec postgres pg_dump -U postgres hive_metastore > metastore_backup.sql

# Restore
docker compose exec -T postgres psql -U postgres hive_metastore < metastore_backup.sql
```

### Limpeza de Tabelas Ã“rfÃ£s

```sql
-- Identificar tabelas sem dados
SELECT 
    t."TBL_NAME",
    s."LOCATION"
FROM "TBLS" t
JOIN "SDS" s ON t."SD_ID" = s."SD_ID"
WHERE s."LOCATION" LIKE 's3a://%';
```

### Validar ConsistÃªncia

```scala
// Spark: Verificar tabelas quebradas
val databases = spark.sql("SHOW DATABASES").collect()
databases.foreach { db =>
  val dbName = db.getString(0)
  println(s"Checking database: $dbName")
  
  try {
    spark.sql(s"USE $dbName")
    val tables = spark.sql("SHOW TABLES").collect()
    
    tables.foreach { table =>
      val tableName = table.getString(1)
      try {
        spark.sql(s"SELECT COUNT(*) FROM $tableName").show()
        println(s"âœ“ $dbName.$tableName OK")
      } catch {
        case e: Exception =>
          println(s"âœ— $dbName.$tableName ERROR: ${e.getMessage}")
      }
    }
  } catch {
    case e: Exception =>
      println(s"âœ— Database $dbName ERROR: ${e.getMessage}")
  }
}
```

## ðŸ“‹ Schema Evolution

### Adicionar Colunas

```sql
-- Trino: Adicionar coluna
ALTER TABLE hive.vendas.produtos 
ADD COLUMN estoque INT;

-- Spark: Com valor default
spark.sql("""
  ALTER TABLE vendas.produtos
  ADD COLUMNS (estoque INT COMMENT 'Quantidade em estoque')
""")
```

### Modificar Tipos

```scala
// Trocar tipo de coluna (criar nova tabela)
spark.sql("""
  CREATE TABLE vendas.produtos_v2 AS
  SELECT 
    id,
    nome,
    categoria,
    CAST(preco AS DOUBLE) as preco,
    data_criacao
  FROM vendas.produtos
""")

// Dropar antiga e renomear
spark.sql("DROP TABLE vendas.produtos")
spark.sql("ALTER TABLE vendas.produtos_v2 RENAME TO vendas.produtos")
```

## ðŸŽ¯ Best Practices

### 1. Nomenclatura
```
âœ“ databases: minÃºsculo, underscores (vendas_online)
âœ“ tabelas: minÃºsculo, underscores (vendas_silver)
âœ“ colunas: minÃºsculo, underscores (data_venda)
âœ— evitar: CamelCase, espaÃ§os, caracteres especiais
```

### 2. Particionamento
```
âœ“ Particione por data se queries filtram por perÃ­odo
âœ“ Use granularidade adequada (dia/mÃªs/ano)
âœ“ Evite muitas partiÃ§Ãµes pequenas (< 1GB cada)
âœ— NÃ£o particione tabelas pequenas (< 1GB total)
```

### 3. Formatos
```
âœ“ Parquet/ORC para dados analÃ­ticos
âœ“ CompressÃ£o Snappy (balanÃ§o speed/ratio)
âœ“ Schema explÃ­cito (nÃ£o inferir)
âœ— CSV apenas para ingestÃ£o inicial
```

### 4. OrganizaÃ§Ã£o
```
vendas/              # Database
â”œâ”€â”€ raw/             # Bronze (tabelas externas)
â”œâ”€â”€ staging/         # Silver (tabelas gerenciadas)
â””â”€â”€ analytics/       # Gold (tabelas agregadas)
```

## ðŸ†˜ Troubleshooting

### Erro: "Table not found"
```bash
# Verificar se metastore estÃ¡ rodando
docker compose ps hive-metastore

# Verificar logs
docker compose logs hive-metastore

# Reparar tabela
spark.sql("MSCK REPAIR TABLE vendas.tabela_nome")
```

### PartiÃ§Ãµes nÃ£o aparecem
```scala
// Atualizar partiÃ§Ãµes automaticamente
spark.sql("MSCK REPAIR TABLE vendas.vendas_particionada")

// Ou adicionar manualmente
spark.sql("""
  ALTER TABLE vendas.vendas_particionada
  ADD PARTITION (ano=2025, mes=1)
  LOCATION 's3a://silver/vendas/particionada/ano=2025/mes=1/'
""")
```

### Schema desatualizado
```scala
// Refresh cache
spark.sql("REFRESH TABLE vendas.produtos")

// Invalidar cache
spark.catalog.refreshTable("vendas.produtos")
```

## ðŸŽ“ PrÃ³ximos Passos

- **APIs REST/JDBC:** `07-apis-rest-jdbc.md`
- **Casos de uso prÃ¡ticos:** `08-casos-uso-praticos.md`
- **OtimizaÃ§Ã£o de queries:** DocumentaÃ§Ã£o Trino/Spark
