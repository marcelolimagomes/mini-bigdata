# ğŸš€ Mini Big Data Platform

<div align="center">

![Big Data](https://img.shields.io/badge/Big%20Data-Platform-blue?style=for-the-badge)
![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

**Uma stack Big Data completa e enxuta para desenvolvimento local**

[CaracterÃ­sticas](#-caracterÃ­sticas) â€¢
[Componentes](#-componentes) â€¢
[Quick Start](#-quick-start) â€¢
[DocumentaÃ§Ã£o](#-documentaÃ§Ã£o) â€¢
[Exemplos](#-exemplos-prÃ¡ticos)

</div>

---

## ğŸ“– Sobre o Projeto

O **Mini Big Data Platform** Ã© uma prova de conceito (PoC) de uma arquitetura Big Data moderna, completa e escalÃ¡vel, projetada para rodar localmente via Docker Compose. Este projeto foi desenvolvido com o objetivo de facilitar o aprendizado, experimentaÃ§Ã£o e desenvolvimento de soluÃ§Ãµes de dados sem a necessidade de infraestrutura cloud.

### ğŸ¯ Objetivo

Fornecer um ambiente Big Data **totalmente funcional** que pode ser executado em uma mÃ¡quina local, permitindo que desenvolvedores, engenheiros de dados e entusiastas possam:

- ğŸ§ª **Experimentar** tecnologias Big Data sem custos de cloud
- ğŸ“š **Aprender** arquiteturas modernas de dados (Data Lake, Lakehouse)
- ğŸ”¬ **Desenvolver** e testar pipelines ETL/ELT
- ğŸ“ **Ensinar** conceitos de engenharia de dados
- ğŸš€ **Prototipar** soluÃ§Ãµes antes de deployar em produÃ§Ã£o

### âš¡ CaracterÃ­sticas

- âœ… **Arquitetura Completa**: Storage, processamento, catÃ¡logo, orquestraÃ§Ã£o e visualizaÃ§Ã£o
- âœ… **Totalmente Containerizado**: Infraestrutura como cÃ³digo com Docker Compose
- âœ… **Production-Ready**: Mesmas tecnologias usadas em ambientes corporativos
- âœ… **S3-Compatible**: Utiliza MinIO como alternativa local ao AWS S3
- âœ… **IntegraÃ§Ã£o Nativa**: Todos os componentes comunicam-se nativamente
- âœ… **PersistÃªncia de Dados**: Dados salvos em disco externo para seguranÃ§a
- âœ… **FÃ¡cil Setup**: Scripts automatizados para configuraÃ§Ã£o inicial

## ï¿½ï¸ Componentes

| Componente | Tecnologia | Porta | DescriÃ§Ã£o |
|------------|-----------|-------|-----------|
| **Object Storage** | MinIO | 9000, 9001 | Armazenamento S3-compatible para Data Lake |
| **Orquestrador ETL** | Apache Airflow | 8080 | OrquestraÃ§Ã£o de workflows e pipelines |
| **Processamento** | Apache Spark | 8081, 7077 | Engine de processamento distribuÃ­do (PySpark) |
| **Query Engine** | Trino | 8085 | SQL distribuÃ­do com acesso JDBC/REST |
| **Metastore** | Hive Metastore | 9083 | CatÃ¡logo de dados centralizado |
| **Database** | PostgreSQL | 5432 | Banco relacional para metadados |
| **BI/Dashboards** | Apache Superset | 8088 | Plataforma de visualizaÃ§Ã£o e dashboards |

## ğŸ—ï¸ Arquitetura

A plataforma segue uma arquitetura em camadas (layered architecture), separando responsabilidades e permitindo escalabilidade horizontal:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ğŸ“Š Camada de ApresentaÃ§Ã£o                  â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Apache Superset  â”‚              â”‚  Acesso Externo  â”‚     â”‚
â”‚  â”‚   (Dashboards)   â”‚              â”‚   JDBC/REST API  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           â”‚      ğŸ” Camada de Acesso         â”‚              â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                        â”‚                                    â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                  â”‚   Trino    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚                  â”‚  SQL Engineâ”‚          â”‚                  â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚                        â”‚           â”‚    Hive    â”‚           â”‚
â”‚                        â”‚           â”‚ Metastore  â”‚           â”‚
â”‚                        â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           âš™ï¸ Camada de Processamento                        â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                  â”‚   Spark    â”‚                             â”‚
â”‚                  â”‚  (Workers) â”‚                             â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                        â”‚                                    â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                  â”‚  Airflow   â”‚                             â”‚
â”‚                  â”‚(Orchestrator)                            â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ğŸ’¾ Camada de Storage                           â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚                  â”‚   MinIO    â”‚    â”‚ PostgreSQL   â”‚         â”‚
â”‚                  â”‚ (Data Lake)â”‚    â”‚  (Metadata)  â”‚         â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ Fluxo de Dados

1. **IngestÃ£o**: Dados brutos chegam via APIs, arquivos ou streaming
2. **Storage**: Armazenados no MinIO (Bronze layer)
3. **OrquestraÃ§Ã£o**: Airflow agenda e dispara jobs de processamento
4. **Processamento**: Spark processa dados (Silver/Gold layers)
5. **CatalogaÃ§Ã£o**: Hive Metastore mantÃ©m metadados e schemas
6. **Acesso**: Trino permite queries SQL sobre os dados
7. **VisualizaÃ§Ã£o**: Superset cria dashboards e relatÃ³rios

## ğŸ¯ Casos de Uso

### 1. ğŸ“¦ **Pipeline ETL Completo**
- IngestÃ£o de dados de mÃºltiplas fontes (APIs, arquivos, databases)
- TransformaÃ§Ã£o com Spark (limpeza, agregaÃ§Ã£o, enriquecimento)
- Armazenamento em camadas (Bronze â†’ Silver â†’ Gold)
- CatalogaÃ§Ã£o automÃ¡tica via Hive Metastore
- OrquestraÃ§Ã£o com Airflow (scheduling, retry, alertas)

### 2. ğŸ” **Data Lake & Analytics**
- Armazenamento S3-compatible com MinIO
- Queries SQL federadas com Trino
- Acesso via JDBC para ferramentas BI (Power BI, Tableau, Metabase)
- REST API para aplicaÃ§Ãµes e microserviÃ§os
- AnÃ¡lises ad-hoc com SQL Lab (Superset)

### 3. ğŸ“Š **Business Intelligence**
- Dashboards interativos com Apache Superset
- MÃ©tricas em tempo real
- RelatÃ³rios agendados
- Self-service analytics para usuÃ¡rios de negÃ³cio

### 4. ğŸ§ª **Prototipagem e Testes**
- Testar arquiteturas Data Lake/Lakehouse
- Validar pipelines antes de produÃ§Ã£o
- Benchmarking de performance
- Treinamento de equipes

## ğŸš€ Quick Start

### ğŸ“‹ PrÃ©-requisitos

Antes de iniciar, certifique-se de ter:

- **Docker** >= 20.10 ([Instalar](https://docs.docker.com/engine/install/))
- **Docker Compose** >= 2.0 ([Instalar](https://docs.docker.com/compose/install/))
- **Recursos MÃ­nimos**:
  - 8GB RAM disponÃ­vel
  - 20GB espaÃ§o em disco
  - CPU com 4+ cores (recomendado)
- **Portas Livres**: 5432, 7077, 8080-8088, 9000-9001, 9083

### ğŸ”§ InstalaÃ§Ã£o

**1. Clone o repositÃ³rio**

```bash
git clone https://github.com/marcelolimagomes/mini-bigdata.git
cd mini-bigdata
```

**2. Configure o ambiente**

```bash
# Criar estrutura de diretÃ³rios e configuraÃ§Ãµes
./setup.sh
```

> âš ï¸ **Nota**: O script `setup.sh` criarÃ¡ automaticamente a estrutura de diretÃ³rios em `/media/marcelo/dados1/bigdata-docker/`. Ajuste o caminho no script se necessÃ¡rio.

**3. Inicie os serviÃ§os**

```bash
# Iniciar todos os containers em background
docker-compose up -d

# Verificar status dos serviÃ§os
docker-compose ps

# Acompanhar logs (Ctrl+C para sair)
docker-compose logs -f
```

**4. Aguarde a inicializaÃ§Ã£o**

O primeiro start pode levar 3-5 minutos. Aguarde atÃ© que todos os serviÃ§os estejam `healthy`:

```bash
# Verificar saÃºde dos containers
docker-compose ps
```

### ğŸŒ Acesso Ã s Interfaces

### ğŸŒ Acesso Ã s Interfaces

ApÃ³s a inicializaÃ§Ã£o, acesse as interfaces web:

| ServiÃ§o | URL | UsuÃ¡rio | Senha |
|---------|-----|---------|-------|
| **MinIO Console** | http://localhost:9001 | `minioadmin` | `minioadmin123` |
| **Airflow** | http://localhost:8080 | `airflow` | `airflow` |
| **Superset** | http://localhost:8088 | `admin` | `admin` |
| **Trino UI** | http://localhost:8085 | `trino` | *(sem senha)* |
| **Spark Master** | http://localhost:8081 | - | - |

> ğŸ”’ **Importante**: Altere as credenciais padrÃ£o antes de usar em ambientes nÃ£o-locais!

### ï¿½ PersistÃªncia de Dados

Todos os dados sÃ£o persistidos externamente em:

```
/media/marcelo/dados1/bigdata-docker/
â”œâ”€â”€ postgres/      # Metadados do sistema
â”œâ”€â”€ minio/         # Data Lake (arquivos principais)
â”œâ”€â”€ airflow/       # DAGs, logs e plugins
â”œâ”€â”€ hive/          # Warehouse do Hive
â”œâ”€â”€ spark/         # Jobs e checkpoints
â”œâ”€â”€ trino/         # Cache de queries
â””â”€â”€ superset/      # ConfiguraÃ§Ãµes e dashboards
```

ğŸ“– **DocumentaÃ§Ã£o completa**: [STORAGE.md](STORAGE.md)

## ğŸ“š DocumentaÃ§Ã£o

Este projeto possui documentaÃ§Ã£o detalhada para cada componente:

- ğŸ“– **[Guia de InÃ­cio RÃ¡pido](docs/01-guia-inicio-rapido.md)** - Primeiros passos
- ğŸ”„ **[Criando Pipelines Airflow](docs/02-criando-pipelines-airflow.md)** - DAGs e workflows
- âš¡ **[Processamento com Spark](docs/03-processamento-spark.md)** - Jobs PySpark
- ğŸ” **[Consultas com Trino](docs/04-consultas-trino.md)** - SQL distribuÃ­do
- ğŸ“Š **[Dashboards Superset](docs/05-criando-dashboards-superset.md)** - VisualizaÃ§Ãµes
- ğŸ“ **[CatÃ¡logo Hive Metastore](docs/06-catalogo-hive-metastore.md)** - GestÃ£o de metadados
- ğŸ”Œ **[APIs REST e JDBC](docs/07-apis-rest-jdbc.md)** - Conectividade externa
- ğŸ’¼ **[Casos de Uso PrÃ¡ticos](docs/08-casos-uso-praticos.md)** - Exemplos reais

## ğŸ“Š Exemplos PrÃ¡ticos

### ğŸ¬ Pipeline ETL End-to-End

Exemplo completo de um pipeline de dados, do armazenamento Ã  visualizaÃ§Ã£o:

#### **1ï¸âƒ£ Criar Buckets no MinIO (Bronze/Silver/Gold)**

```python
import boto3

# Cliente S3 apontando para MinIO
s3 = boto3.client(
    's3',
    endpoint_url='http://localhost:9000',
    aws_access_key_id='minioadmin',
    aws_secret_access_key='minioadmin123'
)

# Criar camadas do Data Lake
for bucket in ['bronze', 'silver', 'gold']:
    s3.create_bucket(Bucket=bucket)
    print(f'âœ“ Bucket {bucket} criado')
```

#### **2ï¸âƒ£ DAG do Airflow (OrquestraÃ§Ã£o)**

```python
```python
# dags/etl_sales_pipeline.py
from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'etl_sales_pipeline',
    default_args=default_args,
    description='Pipeline ETL de vendas',
    schedule_interval='@daily',
    catchup=False,
    tags=['etl', 'sales', 'production']
) as dag:
    
    # Task 1: Processar dados brutos (Bronze â†’ Silver)
    process_raw_data = SparkSubmitOperator(
        task_id='process_raw_data',
        application='/opt/airflow/jobs/process_sales.py',
        conn_id='spark_default',
        conf={
            'spark.hadoop.fs.s3a.endpoint': 'http://minio:9000',
            'spark.hadoop.fs.s3a.access.key': 'minioadmin',
            'spark.hadoop.fs.s3a.secret.key': 'minioadmin123'
        }
    )
    
    # Task 2: AgregaÃ§Ãµes e mÃ©tricas (Silver â†’ Gold)
    aggregate_data = SparkSubmitOperator(
        task_id='aggregate_data',
        application='/opt/airflow/jobs/aggregate_sales.py',
        conn_id='spark_default',
    )
    
    # Definir ordem de execuÃ§Ã£o
    process_raw_data >> aggregate_data
```

#### **3ï¸âƒ£ Job Spark - Processamento (PySpark)**

```python
```python
# jobs/process_sales.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, year, month, sum as _sum

# Inicializar Spark com configuraÃ§Ãµes S3
spark = SparkSession.builder \
    .appName("Process Sales Data") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin123") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .getOrCreate()

# 1. Ler dados brutos (Bronze)
df_raw = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv("s3a://bronze/sales/raw_data/")

# 2. TransformaÃ§Ãµes (Limpeza e Enriquecimento)
df_clean = df_raw \
    .filter(col("amount") > 0) \
    .withColumn("sale_date", to_date(col("date"), "yyyy-MM-dd")) \
    .withColumn("year", year(col("sale_date"))) \
    .withColumn("month", month(col("sale_date"))) \
    .dropDuplicates(["transaction_id"]) \
    .na.drop()

# 3. Salvar dados processados (Silver)
df_clean.write \
    .mode('overwrite') \
    .partitionBy("year", "month") \
    .parquet("s3a://silver/sales/processed/")

print(f"âœ“ Processados {df_clean.count()} registros")
spark.stop()
```

#### **4ï¸âƒ£ Consulta SQL com Trino**

```sql
-- Conectar via Trino UI (localhost:8085) ou JDBC

-- Criar tabela externa apontando para dados no MinIO
CREATE TABLE IF NOT EXISTS hive.default.sales_processed (
    transaction_id VARCHAR,
    customer_id VARCHAR,
    product_id VARCHAR,
    amount DOUBLE,
    sale_date DATE,
    year INT,
    month INT
)
WITH (
    external_location = 's3a://silver/sales/processed/',
    format = 'PARQUET',
    partitioned_by = ARRAY['year', 'month']
);

-- AnÃ¡lise: Top 10 produtos mais vendidos
SELECT 
    product_id,
    COUNT(*) as total_sales,
    ROUND(SUM(amount), 2) as total_revenue
FROM hive.default.sales_processed
WHERE year = 2025 AND month = 10
GROUP BY product_id
ORDER BY total_revenue DESC
LIMIT 10;
```

#### **5ï¸âƒ£ Dashboard no Superset**

1. Acesse http://localhost:8088
2. **Database** â†’ **+ Database** â†’ Configure conexÃ£o Trino:
   ```
   trino://trino@trino:8085/hive/default
   ```
3. **SQL Lab** â†’ Execute queries ad-hoc
4. **Charts** â†’ Crie visualizaÃ§Ãµes
5. **Dashboards** â†’ Monte painÃ©is interativos

### ğŸ”— Mais Exemplos

Explore exemplos completos no diretÃ³rio [`examples/`](examples/):

- ğŸ“ **[DAGs](examples/dags/)** - Pipelines Airflow prontos
- âš™ï¸ **[Jobs Spark](examples/jobs/)** - Scripts PySpark
- ğŸ“Š **[Queries SQL](examples/queries/)** - Consultas Trino
- ğŸ **[Scripts Python](examples/access_examples.py)** - Acesso programÃ¡tico

## ğŸ”Œ Conectividade e APIs

## ğŸ”Œ Conectividade e APIs

### ğŸ”— JDBC (Trino)

Conecte ferramentas externas (DBeaver, Power BI, Tableau) via JDBC:

```
JDBC URL: jdbc:trino://localhost:8085/hive/default
Driver: io.trino.jdbc.TrinoDriver
Username: trino
Password: (vazio)
```

**Download do Driver**: [Trino JDBC](https://repo1.maven.org/maven2/io/trino/trino-jdbc/)

### ğŸŒ REST API (Trino)

```bash
# Executar query via REST
curl -X POST http://localhost:8085/v1/statement \
  -H "X-Trino-User: trino" \
  -H "X-Trino-Catalog: hive" \
  -H "X-Trino-Schema: default" \
  -d "SELECT * FROM sales_processed LIMIT 10"
```

### ğŸ Python (MinIO/S3)

```python
import boto3
from botocore.client import Config

# Cliente boto3 para MinIO
s3_client = boto3.client(
    's3',
    endpoint_url='http://localhost:9000',
    aws_access_key_id='minioadmin',
    aws_secret_access_key='minioadmin123',
    config=Config(signature_version='s3v4'),
    region_name='us-east-1'
)

# Listar buckets
response = s3_client.list_buckets()
for bucket in response['Buckets']:
    print(f"  - {bucket['Name']}")

# Upload de arquivo
s3_client.upload_file('local_file.csv', 'bronze', 'data/file.csv')
```

### ğŸ Python (Trino)

```python
from trino.dbapi import connect
from trino.auth import BasicAuthentication

# ConexÃ£o com Trino
conn = connect(
    host='localhost',
    port=8085,
    user='trino',
    catalog='hive',
    schema='default',
)

# Executar query
cursor = conn.cursor()
cursor.execute("SELECT COUNT(*) FROM sales_processed")
result = cursor.fetchone()
print(f"Total de registros: {result[0]}")
```

### ï¿½ PySpark (Local)

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Local Spark App") \
    .master("spark://localhost:7077") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin123") \
    .getOrCreate()

# Ler dados do MinIO
df = spark.read.parquet("s3a://silver/sales/processed/")
df.show(10)
```

## ğŸ“ Estrutura do Projeto

## ğŸ“ Estrutura do Projeto

```
mini-bigdata/
â”‚
â”œâ”€â”€ ğŸ“„ docker-compose.yml          # OrquestraÃ§Ã£o de containers
â”œâ”€â”€ ğŸ”§ setup.sh                    # Script de setup inicial
â”œâ”€â”€ ğŸ“‹ requirements.txt            # DependÃªncias Python
â”œâ”€â”€ ğŸ“– README.md                   # Esta documentaÃ§Ã£o
â”œâ”€â”€ ğŸ“˜ QUICKSTART.md              # Guia rÃ¡pido de inÃ­cio
â”œâ”€â”€ ğŸ’¾ STORAGE.md                 # Detalhes sobre persistÃªncia
â”œâ”€â”€ ğŸ”§ TROUBLESHOOTING.md         # SoluÃ§Ã£o de problemas
â”œâ”€â”€ ğŸ“ INSTALL.md                 # Guia de instalaÃ§Ã£o detalhado
â”‚
â”œâ”€â”€ ğŸ“‚ config/                     # ConfiguraÃ§Ãµes dos serviÃ§os
â”‚   â”œâ”€â”€ airflow/                  # Configs Airflow
â”‚   â”œâ”€â”€ spark/                    # spark-defaults.conf
â”‚   â”‚   â””â”€â”€ spark-defaults.conf
â”‚   â”œâ”€â”€ trino/                    # Configs e catÃ¡logos Trino
â”‚   â”‚   â”œâ”€â”€ config.properties
â”‚   â”‚   â””â”€â”€ catalog/
â”‚   â”‚       â”œâ”€â”€ hive.properties
â”‚   â”‚       â””â”€â”€ memory.properties
â”‚   â”œâ”€â”€ hive/                     # Hive Metastore configs
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ metastore-site.xml
â”‚   â”œâ”€â”€ postgres/                 # Scripts de inicializaÃ§Ã£o
â”‚   â”‚   â””â”€â”€ init-databases.sh
â”‚   â””â”€â”€ superset/                 # Superset configs
â”‚       â”œâ”€â”€ init-superset.sh
â”‚       â””â”€â”€ superset_config.py
â”‚
â”œâ”€â”€ ğŸ“‚ data/                       # Volumes Docker (runtime)
â”‚   â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ hive/
â”‚   â”œâ”€â”€ minio/
â”‚   â”œâ”€â”€ postgres/
â”‚   â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ superset/
â”‚   â””â”€â”€ trino/
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                       # DocumentaÃ§Ã£o completa
â”‚   â”œâ”€â”€ 01-guia-inicio-rapido.md
â”‚   â”œâ”€â”€ 02-criando-pipelines-airflow.md
â”‚   â”œâ”€â”€ 03-processamento-spark.md
â”‚   â”œâ”€â”€ 04-consultas-trino.md
â”‚   â”œâ”€â”€ 05-criando-dashboards-superset.md
â”‚   â”œâ”€â”€ 06-catalogo-hive-metastore.md
â”‚   â”œâ”€â”€ 07-apis-rest-jdbc.md
â”‚   â”œâ”€â”€ 08-casos-uso-praticos.md
â”‚   â”œâ”€â”€ INDICE.md
â”‚   â””â”€â”€ senhas.txt                # Credenciais padrÃ£o
â”‚
â””â”€â”€ ğŸ“‚ examples/                   # Exemplos prÃ¡ticos
    â”œâ”€â”€ access_examples.py        # Scripts de acesso
    â”œâ”€â”€ dags/                     # DAGs Airflow exemplo
    â”‚   â””â”€â”€ etl_sales_pipeline.py
    â”œâ”€â”€ jobs/                     # Jobs Spark exemplo
    â”‚   â”œâ”€â”€ process_sales.py
    â”‚   â””â”€â”€ aggregate_sales.py
    â”œâ”€â”€ queries/                  # Queries SQL exemplo
    â”‚   â””â”€â”€ trino_examples.sql
    â”œâ”€â”€ notebooks/                # Jupyter notebooks
    â””â”€â”€ data/                     # Dados de exemplo
```

### ğŸ’¾ Dados Persistidos (Disco Externo)

```
/media/marcelo/dados1/bigdata-docker/
â”œâ”€â”€ postgres/      # ğŸ—„ï¸  Metadados (Airflow, Superset, Hive)
â”œâ”€â”€ minio/         # ğŸ“¦ Data Lake (arquivos principais)
â”œâ”€â”€ airflow/       # ğŸ”„ DAGs, logs, plugins
â”œâ”€â”€ hive/          # ğŸ“š Warehouse do Hive
â”œâ”€â”€ spark/         # âš¡ Jobs, checkpoints, event logs
â”œâ”€â”€ trino/         # ğŸ” Cache de queries
â””â”€â”€ superset/      # ğŸ“Š Dashboards e configuraÃ§Ãµes
```

## ğŸ› ï¸ Comandos Ãšteis

## ğŸ› ï¸ Comandos Ãšteis

### ğŸš€ Gerenciamento do Ambiente

```bash
# Iniciar todos os serviÃ§os
docker-compose up -d

# Parar todos os serviÃ§os (mantÃ©m dados)
docker-compose down

# Parar e remover volumes Docker (âš ï¸ dados em /media/marcelo/dados1/ sÃ£o mantidos)
docker-compose down -v

# Reiniciar ambiente completo
docker-compose restart

# Reiniciar serviÃ§o especÃ­fico
docker-compose restart airflow-webserver
docker-compose restart spark-master
docker-compose restart trino

# Ver status dos containers
docker-compose ps

# Ver estatÃ­sticas de recursos
docker stats
```

### ğŸ“‹ Logs e Debugging

```bash
# Ver logs de todos os serviÃ§os
docker-compose logs -f

# Ver logs de um serviÃ§o especÃ­fico
docker-compose logs -f trino
docker-compose logs -f spark-master
docker-compose logs -f airflow-scheduler

# Ver Ãºltimas 100 linhas
docker-compose logs --tail=100 airflow-webserver

# Acessar shell do container
docker-compose exec airflow-webserver bash
docker-compose exec spark-master bash
docker-compose exec trino /bin/bash
```

### âš™ï¸ Escalabilidade

```bash
# Escalar workers do Airflow
docker-compose up -d --scale airflow-worker=3

# Escalar workers do Spark
docker-compose up -d --scale spark-worker=2

# Verificar workers ativos
docker-compose ps | grep worker
```

### ğŸ’¾ Backup e ManutenÃ§Ã£o

```bash
# Backup completo dos dados
tar -czf backup-bigdata-$(date +%Y%m%d).tar.gz \
  /media/marcelo/dados1/bigdata-docker/

# Backup especÃ­fico (apenas MinIO)
tar -czf backup-minio-$(date +%Y%m%d).tar.gz \
  /media/marcelo/dados1/bigdata-docker/minio/

# Verificar espaÃ§o em disco
du -sh /media/marcelo/dados1/bigdata-docker/*/

# Limpar logs antigos do Airflow
docker-compose exec airflow-scheduler \
  airflow db clean --clean-before-timestamp "$(date -d '30 days ago' '+%Y-%m-%d')" -y

# Limpar cache do Docker
docker system prune -a --volumes
```

### ğŸ§¹ Limpeza e Reset

```bash
# Reset completo (âš ï¸ CUIDADO: remove TODOS os dados)
docker-compose down -v
sudo rm -rf /media/marcelo/dados1/bigdata-docker/
./setup.sh
docker-compose up -d

# Rebuild de um serviÃ§o especÃ­fico
docker-compose build --no-cache hive-metastore
docker-compose up -d hive-metastore
```

## ğŸ”§ CustomizaÃ§Ã£o e ConfiguraÃ§Ã£o

## ğŸ”§ CustomizaÃ§Ã£o e ConfiguraÃ§Ã£o

### ğŸ” Alterar Credenciais PadrÃ£o

Edite as variÃ¡veis no arquivo `.env` ou diretamente no `docker-compose.yml`:

```bash
# MinIO
MINIO_ROOT_USER=seu_usuario
MINIO_ROOT_PASSWORD=SuaSenhaSegura123!

# PostgreSQL
POSTGRES_PASSWORD=postgres_senha_forte

# Airflow
AIRFLOW_WWW_USER_USERNAME=admin
AIRFLOW_WWW_USER_PASSWORD=admin_senha_forte
```

ApÃ³s alterar, reinicie os serviÃ§os:

```bash
docker-compose down
docker-compose up -d
```

### ğŸ“Š Adicionar CatÃ¡logos no Trino

Crie novos arquivos em `config/trino/catalog/`:

**MySQL Catalog** (`config/trino/catalog/mysql.properties`):
```properties
connector.name=mysql
connection-url=jdbc:mysql://mysql-host:3306
connection-user=root
connection-password=senha
```

**PostgreSQL Catalog** (`config/trino/catalog/postgresql.properties`):
```properties
connector.name=postgresql
connection-url=jdbc:postgresql://postgres-host:5432/database
connection-user=postgres
connection-password=senha
```

**Iceberg Catalog** (`config/trino/catalog/iceberg.properties`):
```properties
connector.name=iceberg
hive.metastore.uri=thrift://hive-metastore:9083
iceberg.catalog.type=hive_metastore
```

Reinicie o Trino apÃ³s adicionar catÃ¡logos:

```bash
docker-compose restart trino
```

### âš¡ Ajustar Recursos do Spark

Edite `config/spark/spark-defaults.conf`:

```properties
# MemÃ³ria do driver
spark.driver.memory              2g

# MemÃ³ria do executor
spark.executor.memory            4g
spark.executor.cores             2

# NÃºmero de executores
spark.executor.instances         2

# ConfiguraÃ§Ãµes de shuffle
spark.sql.shuffle.partitions     200
```

### ï¿½ Configurar Paralelismo do Airflow

Edite no `docker-compose.yml`:

```yaml
environment:
  AIRFLOW__CORE__PARALLELISM: 32
  AIRFLOW__CORE__DAG_CONCURRENCY: 16
  AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: 16
```

## ğŸ“Š Formatos de Dados Recomendados

### ğŸ—ï¸ Arquitetura de Data Lake (Medallion)

| Camada | Formato | CompressÃ£o | Uso | Exemplo |
|--------|---------|------------|-----|---------|
| **ğŸ¥‰ Bronze** | JSON, CSV, Avro | gzip, snappy | Dados brutos | `s3a://bronze/raw/` |
| **ğŸ¥ˆ Silver** | Parquet | snappy | Dados limpos | `s3a://silver/processed/` |
| **ğŸ¥‡ Gold** | Parquet, Delta, Iceberg | zstd | Analytics-ready | `s3a://gold/analytics/` |

### ğŸ’¡ RecomendaÃ§Ãµes

- **Bronze**: Mantenha formato original, adicione apenas metadados (ingest_date, source, etc)
- **Silver**: Converta para Parquet, aplique schema, particione por data
- **Gold**: Use Delta/Iceberg para ACID e time-travel, agregue dados por caso de uso

## ğŸ” Monitoramento e Observabilidade

## ğŸ” Monitoramento e Observabilidade

### ğŸ“Š UIs de Monitoramento

| Interface | URL | DescriÃ§Ã£o |
|-----------|-----|-----------|
| **Airflow** | http://localhost:8080 | Status DAGs, task logs, conexÃµes |
| **Spark Master** | http://localhost:8081 | Jobs, stages, executors |
| **Trino** | http://localhost:8085 | Query monitoring, cluster status |
| **MinIO Console** | http://localhost:9001 | Storage metrics, buckets, bandwidth |
| **Superset** | http://localhost:8088 | Dashboard usage, query logs |

### ğŸ“ˆ MÃ©tricas Importantes

**Airflow**:
- DAG run duration
- Task success/failure rate
- Scheduler heartbeat

**Spark**:
- Job duration
- Stages e tasks completed
- Memory e CPU usage

**Trino**:
- Query execution time
- Data read/written
- Active queries

**MinIO**:
- Storage utilizado
- Bandwidth (upload/download)
- Request rate

### ğŸ”” Alertas (Opcional)

Para implementar alertas, considere adicionar:
- **Prometheus** + **Grafana** para mÃ©tricas
- **Alertmanager** para notificaÃ§Ãµes
- ConfiguraÃ§Ã£o de email no Airflow para falhas de DAG

## ğŸ› Troubleshooting

### âŒ Problemas Comuns

<details>
<summary><b>ServiÃ§os nÃ£o sobem / Container em estado "unhealthy"</b></summary>

```bash
# Verificar logs do serviÃ§o especÃ­fico
docker-compose logs trino
docker-compose logs airflow-webserver

# Verificar recursos disponÃ­veis
docker stats

# Recriar containers
docker-compose down
docker-compose up -d --force-recreate
```
</details>

<details>
<summary><b>Airflow nÃ£o conecta ao Spark</b></summary>

1. Verificar conexÃ£o no Airflow Admin â†’ Connections
2. Conn Id: `spark_default`
3. Host: `spark://spark-master:7077`
4. Reiniciar scheduler: `docker-compose restart airflow-scheduler`
</details>

<details>
<summary><b>Trino nÃ£o encontra tabelas</b></summary>

```bash
# Verificar se Hive Metastore estÃ¡ rodando
docker-compose ps hive-metastore

# Conectar ao Trino e verificar catÃ¡logos
docker-compose exec trino trino
> SHOW CATALOGS;
> SHOW SCHEMAS FROM hive;
> SHOW TABLES FROM hive.default;

# Verificar permissÃµes no MinIO
# Acesse http://localhost:9001 e verifique buckets
```
</details>

<details>
<summary><b>Falta de espaÃ§o em disco</b></summary>

```bash
# Verificar uso de disco
df -h /media/marcelo/dados1/

# Ver tamanho por componente
du -sh /media/marcelo/dados1/bigdata-docker/*/

# Limpar logs antigos
docker-compose exec airflow-scheduler airflow db clean --clean-before-timestamp "$(date -d '30 days ago' '+%Y-%m-%d')" -y

# Limpar cache Docker
docker system prune -a
```
</details>

<details>
<summary><b>Porta jÃ¡ em uso</b></summary>

```bash
# Verificar processo usando porta
sudo lsof -i :8080
sudo lsof -i :9000

# Matar processo (exemplo)
sudo kill -9 <PID>

# Ou alterar portas no docker-compose.yml
ports:
  - "8081:8080"  # Muda porta externa
```
</details>

### ğŸ“š DocumentaÃ§Ã£o de Troubleshooting

Para problemas mais complexos, consulte:
- **[TROUBLESHOOTING.md](TROUBLESHOOTING.md)** - Guia completo de soluÃ§Ã£o de problemas
- **[STORAGE.md](STORAGE.md)** - Problemas relacionados a persistÃªncia
- **[SUPERSET-DRIVERS.md](SUPERSET-DRIVERS.md)** - ConfiguraÃ§Ã£o de drivers do Superset
- Issues do projeto no GitHub

## ğŸš€ Roadmap e Melhorias Futuras

## ğŸš€ Roadmap e Melhorias Futuras

### ğŸ¯ PrÃ³ximas ImplementaÃ§Ãµes

- [ ] **Apache Iceberg / Delta Lake** - Table format para ACID transactions
- [ ] **Apache Kafka** - Streaming de dados em tempo real
- [ ] **Jupyter Lab** - Notebooks para anÃ¡lises interativas
- [ ] **dbt (Data Build Tool)** - TransformaÃ§Ãµes SQL como cÃ³digo
- [ ] **Great Expectations** - Data quality e testes
- [ ] **Apache Atlas** - Data governance e lineage
- [ ] **Prometheus + Grafana** - Monitoring avanÃ§ado
- [ ] **Apache Ranger** - Security e controle de acesso
- [ ] **MLflow** - Machine Learning lifecycle
- [ ] **Dagster** - Alternativa moderna ao Airflow

### ï¿½ Ideias de ContribuiÃ§Ã£o

ContribuiÃ§Ãµes sÃ£o bem-vindas! Veja como vocÃª pode ajudar:

1. ğŸ› Reportar bugs via [Issues](https://github.com/marcelolimagomes/mini-bigdata/issues)
2. ğŸ“– Melhorar documentaÃ§Ã£o
3. âœ¨ Adicionar novos exemplos prÃ¡ticos
4. ğŸ”§ Propor otimizaÃ§Ãµes de configuraÃ§Ã£o
5. ğŸ¨ Criar dashboards de exemplo no Superset
6. ğŸ“Š Adicionar datasets de exemplo

## ğŸ“š Recursos de Aprendizado

### ğŸ“– DocumentaÃ§Ã£o Oficial

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [Trino Documentation](https://trino.io/docs/current/)
- [MinIO Documentation](https://min.io/docs/minio/linux/index.html)
- [Apache Superset Documentation](https://superset.apache.org/docs/intro)
- [Hive Metastore](https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+Administration)

### ğŸ“ Tutoriais e Cursos

- [Fundamentals of Data Engineering (Joe Reis)](https://www.oreilly.com/library/view/fundamentals-of-data/9781098108298/)
- [Data Engineering Cookbook](https://github.com/andkret/Cookbook)
- [Awesome Data Engineering](https://github.com/igorbarinov/awesome-data-engineering)

### ğŸŒ Comunidades

- [Data Engineering Brasil](https://www.linkedin.com/groups/12345678/)
- [Stack Overflow - Tags: airflow, spark, trino](https://stackoverflow.com/)
- [Reddit r/dataengineering](https://www.reddit.com/r/dataengineering/)

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o muito bem-vindas! Para contribuir:

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add: nova feature incrÃ­vel'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

### ğŸ“‹ Guidelines

- Mantenha o cÃ³digo limpo e documentado
- Adicione exemplos prÃ¡ticos quando possÃ­vel
- Atualize a documentaÃ§Ã£o relevante
- Teste suas mudanÃ§as antes de submeter PR

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a **MIT License** - veja o arquivo [LICENSE](LICENSE) para detalhes.

VocÃª Ã© livre para:
- âœ… Usar comercialmente
- âœ… Modificar
- âœ… Distribuir
- âœ… Uso privado

## ğŸ‘¤ Autor

**Marcelo Lima Gomes**

- ğŸŒ GitHub: [@marcelolimagomes](https://github.com/marcelolimagomes)
- ğŸ“§ Email: marcelolimagomes@gmail.com
- ğŸ’¼ LinkedIn: [Marcelo Lima Gomes](https://www.linkedin.com/in/marcelolimagomes/)

## â­ Agradecimentos

Este projeto foi inspirado por diversas fontes da comunidade open-source:

- Apache Software Foundation
- Trino Community
- MinIO Project
- Docker Community

## ğŸ™ Apoie o Projeto

Se este projeto foi Ãºtil para vocÃª:

- â­ DÃª uma estrela no GitHub
- ğŸ”€ FaÃ§a um fork
- ğŸ“¢ Compartilhe com sua rede
- ğŸ’¬ DÃª feedback via Issues
- ğŸ¤ Contribua com melhorias

## ğŸ“ Suporte

- ğŸ› **Bugs**: Abra uma [issue](https://github.com/marcelolimagomes/mini-bigdata/issues)
- ğŸ’¬ **DÃºvidas**: Use as [Discussions](https://github.com/marcelolimagomes/mini-bigdata/discussions)
- ğŸ“§ **Contato Direto**: marcelolimagomes@gmail.com

---

<div align="center">

**Desenvolvido com â¤ï¸ para a comunidade de Big Data e DevOps**

[![GitHub](https://img.shields.io/badge/GitHub-marcelolimagomes-181717?style=for-the-badge&logo=github)](https://github.com/marcelolimagomes)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge)](https://opensource.org/licenses/MIT)

**[â¬† Voltar ao topo](#-mini-big-data-platform)**

</div>
