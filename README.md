# üöÄ Mini Big Data Platform

<div align="center">

![Big Data](https://img.shields.io/badge/Big%20Data-Platform-blue?style=for-the-badge)
![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

**Uma stack Big Data completa e enxuta para desenvolvimento local**

[Caracter√≠sticas](#-caracter√≠sticas) ‚Ä¢
[Componentes](#-componentes) ‚Ä¢
[Quick Start](#-quick-start) ‚Ä¢
[Documenta√ß√£o](#-documenta√ß√£o) ‚Ä¢
[Exemplos](#-exemplos-pr√°ticos)

</div>

---

## üìñ Sobre o Projeto

O **Mini Big Data Platform** √© uma prova de conceito (PoC) de uma arquitetura Big Data moderna, completa e escal√°vel, projetada para rodar localmente via Docker Compose. Este projeto foi desenvolvido com o objetivo de facilitar o aprendizado, experimenta√ß√£o e desenvolvimento de solu√ß√µes de dados sem a necessidade de infraestrutura cloud.

### üéØ Objetivo

Fornecer um ambiente Big Data **totalmente funcional** que pode ser executado em uma m√°quina local, permitindo que desenvolvedores, engenheiros de dados e entusiastas possam:

- üß™ **Experimentar** tecnologias Big Data sem custos de cloud
- üìö **Aprender** arquiteturas modernas de dados (Data Lake, Lakehouse)
- üî¨ **Desenvolver** e testar pipelines ETL/ELT
- üéì **Ensinar** conceitos de engenharia de dados
- üöÄ **Prototipar** solu√ß√µes antes de deployar em produ√ß√£o

### ‚ö° Caracter√≠sticas

- ‚úÖ **Setup Automatizado**: 1 comando para configurar tudo (~8 minutos)
- ‚úÖ **Arquitetura Completa**: Storage, processamento, cat√°logo, orquestra√ß√£o e visualiza√ß√£o
- ‚úÖ **Totalmente Containerizado**: Infraestrutura como c√≥digo com Docker Compose
- ‚úÖ **Production-Ready**: Mesmas tecnologias usadas em ambientes corporativos
- ‚úÖ **S3-Compatible**: Utiliza MinIO como alternativa local ao AWS S3
- ‚úÖ **Integra√ß√£o Nativa**: Todos os componentes comunicam-se nativamente
- ‚úÖ **Persist√™ncia de Dados**: Dados salvos em disco externo para seguran√ßa
- ‚úÖ **Valida√ß√£o Autom√°tica**: Scripts de valida√ß√£o de todos os servi√ßos

## ÔøΩÔ∏è Componentes

| Componente | Tecnologia | Porta | Descri√ß√£o |
|------------|-----------|-------|-----------|
| **Object Storage** | MinIO | 9000, 9001 | Armazenamento S3-compatible para Data Lake |
| **Cache/Results** | Redis | 6379 | Cache e Results Backend para Superset |
| **Orquestrador ETL** | Apache Airflow | 8080 | Orquestra√ß√£o de workflows e pipelines |
| **Processamento** | Apache Spark | 8081, 7077 | Engine de processamento distribu√≠do (PySpark) |
| **Query Engine** | Trino | 8085 | SQL distribu√≠do com acesso JDBC/REST |
| **Metastore** | Hive Metastore | 9083 | Cat√°logo de dados centralizado |
| **Database** | PostgreSQL | 5432 | Banco relacional para metadados |
| **BI/Dashboards** | Apache Superset | 8088 | Plataforma de visualiza√ß√£o e dashboards |

> üìù **Nota**: O Apache Superset est√° configurado com **Redis Results Backend**, permitindo execu√ß√£o de queries SQL via API e armazenamento de resultados tempor√°rios.

## üèóÔ∏è Arquitetura

A plataforma segue uma arquitetura em camadas (layered architecture), separando responsabilidades e permitindo escalabilidade horizontal:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  üìä Camada de Apresenta√ß√£o                  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ Apache Superset  ‚îÇ              ‚îÇ  Acesso Externo  ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ   (Dashboards)   ‚îÇ              ‚îÇ   JDBC/REST API  ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                                 ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           ‚îÇ      üîç Camada de Acesso         ‚îÇ              ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îÇ                        ‚îÇ                                    ‚îÇ
‚îÇ                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                             ‚îÇ
‚îÇ                  ‚îÇ   Trino    ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
‚îÇ                  ‚îÇ  SQL Engine‚îÇ          ‚îÇ                  ‚îÇ
‚îÇ                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ                        ‚îÇ           ‚îÇ    Hive    ‚îÇ           ‚îÇ
‚îÇ                        ‚îÇ           ‚îÇ Metastore  ‚îÇ           ‚îÇ
‚îÇ                        ‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           ‚öôÔ∏è Camada de Processamento                        ‚îÇ
‚îÇ                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                             ‚îÇ
‚îÇ                  ‚îÇ   Spark    ‚îÇ                             ‚îÇ
‚îÇ                  ‚îÇ  (Workers) ‚îÇ                             ‚îÇ
‚îÇ                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                             ‚îÇ
‚îÇ                        ‚îÇ                                    ‚îÇ
‚îÇ                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                             ‚îÇ
‚îÇ                  ‚îÇ  Airflow   ‚îÇ                             ‚îÇ
‚îÇ                  ‚îÇ(Orchestrator)                            ‚îÇ
‚îÇ                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              üíæ Camada de Storage                           ‚îÇ
‚îÇ                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ                  ‚îÇ   MinIO    ‚îÇ    ‚îÇ PostgreSQL   ‚îÇ         ‚îÇ
‚îÇ                  ‚îÇ (Data Lake)‚îÇ    ‚îÇ  (Metadata)  ‚îÇ         ‚îÇ
‚îÇ                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üîÑ Fluxo de Dados

1. **Ingest√£o**: Dados brutos chegam via APIs, arquivos ou streaming
2. **Storage**: Armazenados no MinIO (Bronze layer)
3. **Orquestra√ß√£o**: Airflow agenda e dispara jobs de processamento
4. **Processamento**: Spark processa dados (Silver/Gold layers)
5. **Cataloga√ß√£o**: Hive Metastore mant√©m metadados e schemas
6. **Acesso**: Trino permite queries SQL sobre os dados
7. **Visualiza√ß√£o**: Superset cria dashboards e relat√≥rios

## üéØ Casos de Uso

### 1. üì¶ **Pipeline ETL Completo**
- Ingest√£o de dados de m√∫ltiplas fontes (APIs, arquivos, databases)
- Transforma√ß√£o com Spark (limpeza, agrega√ß√£o, enriquecimento)
- Armazenamento em camadas (Bronze ‚Üí Silver ‚Üí Gold)
- Cataloga√ß√£o autom√°tica via Hive Metastore
- Orquestra√ß√£o com Airflow (scheduling, retry, alertas)

### 2. üîç **Data Lake & Analytics**
- Armazenamento S3-compatible com MinIO
- Queries SQL federadas com Trino
- Acesso via JDBC para ferramentas BI (Power BI, Tableau, Metabase)
- REST API para aplica√ß√µes e microservi√ßos
- An√°lises ad-hoc com SQL Lab (Superset)

### 3. üìä **Business Intelligence**
- Dashboards interativos com Apache Superset
- M√©tricas em tempo real
- Relat√≥rios agendados
- Self-service analytics para usu√°rios de neg√≥cio

### 4. üß™ **Prototipagem e Testes**
- Testar arquiteturas Data Lake/Lakehouse
- Validar pipelines antes de produ√ß√£o
- Benchmarking de performance
- Treinamento de equipes

## üöÄ Quick Start

### üìã Pr√©-requisitos

Antes de iniciar, certifique-se de ter:

- **Docker** >= 20.10 ([Instalar](https://docs.docker.com/engine/install/))
- **Docker Compose** >= 2.0 ([Instalar](https://docs.docker.com/compose/install/))
- **Python 3** >= 3.8 (para scripts de automa√ß√£o)
- **Recursos M√≠nimos**:
  - 8GB RAM dispon√≠vel
  - 20GB espa√ßo em disco
  - CPU com 4+ cores (recomendado)
- **Portas Livres**: 5432, 6379, 7077, 8080-8088, 9000-9001, 9083

### ‚ö° Setup Automatizado (Recomendado)

**Op√ß√£o 1: Setup completo com 1 comando** üéØ

```bash
# Clonar reposit√≥rio
git clone https://github.com/marcelolimagomes/mini-bigdata.git
cd mini-bigdata

# Executar setup automatizado
python3 scripts/full_setup.py
```

**Tempo estimado:** ~8 minutos  
**O que faz:**
- ‚úÖ Cria estrutura de diret√≥rios
- ‚úÖ Limpa ambiente Docker
- ‚úÖ Constr√≥i imagens personalizadas
- ‚úÖ Inicia todos os servi√ßos em ordem
- ‚úÖ Valida health checks
- ‚úÖ Configura MinIO, Trino e Superset

**Op√ß√£o 2: Script Shell** (alternativa)

```bash
./scripts/shell/full-setup.sh
```

### üîß Setup Manual (Avan√ßado)

Se preferir ter mais controle sobre o processo:

**1. Criar estrutura de diret√≥rios**

```bash
sudo mkdir -p /media/marcelo/dados1/bigdata-docker/{postgres,minio,hive,trino,superset,redis,airflow/{dags,logs,plugins},spark/{master,worker}}
sudo chmod -R 755 /media/marcelo/dados1/bigdata-docker
```

> ‚ö†Ô∏è **Nota**: Ajuste o caminho `/media/marcelo/dados1/bigdata-docker` se necess√°rio no arquivo `docker-compose.yml`.

**2. Construir imagens personalizadas**

```bash
docker compose build hive-metastore trino
```

**3. Iniciar servi√ßos**

```bash
# Iniciar todos os containers em background
docker-compose up -d

# Verificar status dos servi√ßos
docker-compose ps

# Acompanhar logs (Ctrl+C para sair)
docker-compose logs -f
```

**4. Aguarde a inicializa√ß√£o**

O primeiro start pode levar 3-5 minutos. Aguarde at√© que todos os servi√ßos estejam `healthy`:

```bash
# Verificar sa√∫de dos containers
docker-compose ps

# OU validar todos os servi√ßos com script Python
source .venv/bin/activate  # (se configurou o ambiente virtual)
python3 scripts/validate_all_services.py
```

### üåê Acesso √†s Interfaces

### üåê Acesso √†s Interfaces

Ap√≥s a inicializa√ß√£o, acesse as interfaces web:

| Servi√ßo | URL | Usu√°rio | Senha |
|---------|-----|---------|-------|
| **MinIO Console** | http://localhost:9001 | `minioadmin` | `minioadmin123` |
| **Airflow** | http://localhost:8080 | `airflow` | `airflow` |
| **Superset** | http://localhost:8088 | `admin` | `admin` |
| **Trino UI** | http://localhost:8085 | `trino` | *(sem senha)* |
| **Spark Master** | http://localhost:8081 | - | - |

> üîí **Importante**: Altere as credenciais padr√£o antes de usar em ambientes n√£o-locais!

### ÔøΩ Persist√™ncia de Dados

Todos os dados s√£o persistidos externamente em:

```
/media/marcelo/dados1/bigdata-docker/
‚îú‚îÄ‚îÄ postgres/      # Metadados do sistema
‚îú‚îÄ‚îÄ minio/         # Data Lake (arquivos principais)
‚îú‚îÄ‚îÄ airflow/       # DAGs, logs e plugins
‚îú‚îÄ‚îÄ hive/          # Warehouse do Hive
‚îú‚îÄ‚îÄ spark/         # Jobs e checkpoints
‚îú‚îÄ‚îÄ trino/         # Cache de queries
‚îî‚îÄ‚îÄ superset/      # Configura√ß√µes e dashboards
```

üìñ **Documenta√ß√£o completa**: [STORAGE.md](STORAGE.md)

## üìö Documenta√ß√£o

Este projeto possui documenta√ß√£o detalhada para cada componente:

- üìñ **[Guia de In√≠cio R√°pido](docs/01-guia-inicio-rapido.md)** - Primeiros passos
- üîÑ **[Criando Pipelines Airflow](docs/02-criando-pipelines-airflow.md)** - DAGs e workflows
- ‚ö° **[Processamento com Spark](docs/03-processamento-spark.md)** - Jobs PySpark
- üîç **[Consultas com Trino](docs/04-consultas-trino.md)** - SQL distribu√≠do
- üìä **[Dashboards Superset](docs/05-criando-dashboards-superset.md)** - Visualiza√ß√µes
- üìÅ **[Cat√°logo Hive Metastore](docs/06-catalogo-hive-metastore.md)** - Gest√£o de metadados
- üîå **[APIs REST e JDBC](docs/07-apis-rest-jdbc.md)** - Conectividade externa
- üíº **[Casos de Uso Pr√°ticos](docs/08-casos-uso-praticos.md)** - Exemplos reais
- ‚öôÔ∏è **[Configura√ß√£o Automatizada](docs/09-configuracao-automatizada.md)** - Scripts de setup

### üìä Relat√≥rios de Testes e Valida√ß√£o

- üìã **[Valida√ß√£o API Superset](docs/reports/VALIDACAO_SUPERSET_API.md)** - Testes das APIs REST
- üìÑ **[Relat√≥rio Final de Testes](docs/reports/RELATORIO-FINAL-TESTES-API-SUPERSET.md)** - Resultados completos
- üìñ **[README Testes API](docs/reports/README-TESTES-API-SUPERSET.md)** - Guia de testes

## üìä Exemplos Pr√°ticos

### üé¨ Pipeline ETL End-to-End

Exemplo completo de um pipeline de dados, do armazenamento √† visualiza√ß√£o:

#### **1Ô∏è‚É£ Criar Buckets no MinIO (Bronze/Silver/Gold)**

```python
import boto3

# Cliente S3 apontando para MinIO
s3 = boto3.client(
    's3',
    endpoint_url='http://localhost:9000',
    aws_access_key_id='minioadmin',
    aws_secret_access_key='minioadmin123'
)

# Criar camadas do Data Lake
for bucket in ['bronze', 'silver', 'gold']:
    s3.create_bucket(Bucket=bucket)
    print(f'‚úì Bucket {bucket} criado')
```

#### **2Ô∏è‚É£ DAG do Airflow (Orquestra√ß√£o)**

```python
```python
# dags/etl_sales_pipeline.py
from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'etl_sales_pipeline',
    default_args=default_args,
    description='Pipeline ETL de vendas',
    schedule_interval='@daily',
    catchup=False,
    tags=['etl', 'sales', 'production']
) as dag:
    
    # Task 1: Processar dados brutos (Bronze ‚Üí Silver)
    process_raw_data = SparkSubmitOperator(
        task_id='process_raw_data',
        application='/opt/airflow/jobs/process_sales.py',
        conn_id='spark_default',
        conf={
            'spark.hadoop.fs.s3a.endpoint': 'http://minio:9000',
            'spark.hadoop.fs.s3a.access.key': 'minioadmin',
            'spark.hadoop.fs.s3a.secret.key': 'minioadmin123'
        }
    )
    
    # Task 2: Agrega√ß√µes e m√©tricas (Silver ‚Üí Gold)
    aggregate_data = SparkSubmitOperator(
        task_id='aggregate_data',
        application='/opt/airflow/jobs/aggregate_sales.py',
        conn_id='spark_default',
    )
    
    # Definir ordem de execu√ß√£o
    process_raw_data >> aggregate_data
```

#### **3Ô∏è‚É£ Job Spark - Processamento (PySpark)**

```python
```python
# jobs/process_sales.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, year, month, sum as _sum

# Inicializar Spark com configura√ß√µes S3
spark = SparkSession.builder \
    .appName("Process Sales Data") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin123") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .getOrCreate()

# 1. Ler dados brutos (Bronze)
df_raw = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv("s3a://bronze/sales/raw_data/")

# 2. Transforma√ß√µes (Limpeza e Enriquecimento)
df_clean = df_raw \
    .filter(col("amount") > 0) \
    .withColumn("sale_date", to_date(col("date"), "yyyy-MM-dd")) \
    .withColumn("year", year(col("sale_date"))) \
    .withColumn("month", month(col("sale_date"))) \
    .dropDuplicates(["transaction_id"]) \
    .na.drop()

# 3. Salvar dados processados (Silver)
df_clean.write \
    .mode('overwrite') \
    .partitionBy("year", "month") \
    .parquet("s3a://silver/sales/processed/")

print(f"‚úì Processados {df_clean.count()} registros")
spark.stop()
```

#### **4Ô∏è‚É£ Consulta SQL com Trino**

```sql
-- Conectar via Trino UI (localhost:8085) ou JDBC

-- Criar tabela externa apontando para dados no MinIO
CREATE TABLE IF NOT EXISTS hive.default.sales_processed (
    transaction_id VARCHAR,
    customer_id VARCHAR,
    product_id VARCHAR,
    amount DOUBLE,
    sale_date DATE,
    year INT,
    month INT
)
WITH (
    external_location = 's3a://silver/sales/processed/',
    format = 'PARQUET',
    partitioned_by = ARRAY['year', 'month']
);

-- An√°lise: Top 10 produtos mais vendidos
SELECT 
    product_id,
    COUNT(*) as total_sales,
    ROUND(SUM(amount), 2) as total_revenue
FROM hive.default.sales_processed
WHERE year = 2025 AND month = 10
GROUP BY product_id
ORDER BY total_revenue DESC
LIMIT 10;
```

#### **5Ô∏è‚É£ Dashboard no Superset**

1. Acesse http://localhost:8088
2. **Database** ‚Üí **+ Database** ‚Üí Configure conex√£o Trino:
   ```
   trino://trino@trino:8085/hive/default
   ```
3. **SQL Lab** ‚Üí Execute queries ad-hoc
4. **Charts** ‚Üí Crie visualiza√ß√µes
5. **Dashboards** ‚Üí Monte pain√©is interativos

### üîó Mais Exemplos

Explore exemplos completos no diret√≥rio [`examples/`](examples/):

- üìù **[DAGs](examples/dags/)** - Pipelines Airflow prontos
- ‚öôÔ∏è **[Jobs Spark](examples/jobs/)** - Scripts PySpark
- üìä **[Queries SQL](examples/queries/)** - Consultas Trino
- üêç **[Scripts Python](examples/access_examples.py)** - Acesso program√°tico

## üîå Conectividade e APIs

## üîå Conectividade e APIs

### üîó JDBC (Trino)

Conecte ferramentas externas (DBeaver, Power BI, Tableau) via JDBC:

```
JDBC URL: jdbc:trino://localhost:8085/hive/default
Driver: io.trino.jdbc.TrinoDriver
Username: trino
Password: (vazio)
```

**Download do Driver**: [Trino JDBC](https://repo1.maven.org/maven2/io/trino/trino-jdbc/)

### üåê REST API (Trino)

```bash
# Executar query via REST
curl -X POST http://localhost:8085/v1/statement \
  -H "X-Trino-User: trino" \
  -H "X-Trino-Catalog: hive" \
  -H "X-Trino-Schema: default" \
  -d "SELECT * FROM sales_processed LIMIT 10"
```

### üêç Python (MinIO/S3)

```python
import boto3
from botocore.client import Config

# Cliente boto3 para MinIO
s3_client = boto3.client(
    's3',
    endpoint_url='http://localhost:9000',
    aws_access_key_id='minioadmin',
    aws_secret_access_key='minioadmin123',
    config=Config(signature_version='s3v4'),
    region_name='us-east-1'
)

# Listar buckets
response = s3_client.list_buckets()
for bucket in response['Buckets']:
    print(f"  - {bucket['Name']}")

# Upload de arquivo
s3_client.upload_file('local_file.csv', 'bronze', 'data/file.csv')
```

### üêç Python (Trino)

```python
from trino.dbapi import connect
from trino.auth import BasicAuthentication

# Conex√£o com Trino
conn = connect(
    host='localhost',
    port=8085,
    user='trino',
    catalog='hive',
    schema='default',
)

# Executar query
cursor = conn.cursor()
cursor.execute("SELECT COUNT(*) FROM sales_processed")
result = cursor.fetchone()
print(f"Total de registros: {result[0]}")
```

### ÔøΩ PySpark (Local)

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("Local Spark App") \
    .master("spark://localhost:7077") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin123") \
    .getOrCreate()

# Ler dados do MinIO
df = spark.read.parquet("s3a://silver/sales/processed/")
df.show(10)
```

## üìÅ Estrutura do Projeto

## üìÅ Estrutura do Projeto

```
mini-bigdata/
‚îÇ
‚îú‚îÄ‚îÄ üìÑ docker-compose.yml          # Orquestra√ß√£o de containers
‚îú‚îÄ‚îÄ  requirements.txt            # Depend√™ncias Python
‚îú‚îÄ‚îÄ üìñ README.md                   # Esta documenta√ß√£o
‚îú‚îÄ‚îÄ üìò QUICKSTART.md              # Guia r√°pido de in√≠cio
‚îú‚îÄ‚îÄ üíæ STORAGE.md                 # Detalhes sobre persist√™ncia
‚îú‚îÄ‚îÄ üîß TROUBLESHOOTING.md         # Solu√ß√£o de problemas
‚îú‚îÄ‚îÄ üìù INSTALL.md                 # Guia de instala√ß√£o detalhado
‚îÇ
‚îú‚îÄ‚îÄ üìÇ config/                     # Configura√ß√µes dos servi√ßos
‚îÇ   ‚îú‚îÄ‚îÄ airflow/                  # Configs Airflow
‚îÇ   ‚îú‚îÄ‚îÄ spark/                    # spark-defaults.conf
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ spark-defaults.conf
‚îÇ   ‚îú‚îÄ‚îÄ trino/                    # Configs e cat√°logos Trino
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.properties
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ catalog/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ hive.properties
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ memory.properties
‚îÇ   ‚îú‚îÄ‚îÄ hive/                     # Hive Metastore configs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metastore-site.xml
‚îÇ   ‚îú‚îÄ‚îÄ postgres/                 # Scripts de inicializa√ß√£o
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ init-databases.sh
‚îÇ   ‚îî‚îÄ‚îÄ superset/                 # Superset configs
‚îÇ       ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ       ‚îú‚îÄ‚îÄ init-superset.sh
‚îÇ       ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ       ‚îú‚îÄ‚îÄ superset_config.py    # Configura√ß√£o com Redis results backend
‚îÇ       ‚îî‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ üìÇ data/                       # Volumes Docker (runtime)
‚îÇ   ‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îú‚îÄ‚îÄ hive/
‚îÇ   ‚îú‚îÄ‚îÄ minio/
‚îÇ   ‚îú‚îÄ‚îÄ postgres/
‚îÇ   ‚îú‚îÄ‚îÄ spark/
‚îÇ   ‚îú‚îÄ‚îÄ superset/
‚îÇ   ‚îî‚îÄ‚îÄ trino/
‚îÇ
‚îú‚îÄ‚îÄ üìÇ docs/                       # Documenta√ß√£o completa
‚îÇ   ‚îú‚îÄ‚îÄ 01-guia-inicio-rapido.md
‚îÇ   ‚îú‚îÄ‚îÄ 02-criando-pipelines-airflow.md
‚îÇ   ‚îú‚îÄ‚îÄ 03-processamento-spark.md
‚îÇ   ‚îú‚îÄ‚îÄ 04-consultas-trino.md
‚îÇ   ‚îú‚îÄ‚îÄ 05-criando-dashboards-superset.md
‚îÇ   ‚îú‚îÄ‚îÄ 06-catalogo-hive-metastore.md
‚îÇ   ‚îú‚îÄ‚îÄ 07-apis-rest-jdbc.md
‚îÇ   ‚îú‚îÄ‚îÄ 08-casos-uso-praticos.md
‚îÇ   ‚îú‚îÄ‚îÄ 09-configuracao-automatizada.md
‚îÇ   ‚îú‚îÄ‚îÄ INDICE.md
‚îÇ   ‚îú‚îÄ‚îÄ senhas.txt                # Credenciais padr√£o
‚îÇ   ‚îú‚îÄ‚îÄ SUPERSET-v5-GUIA.md       # Guia Apache Superset v5
‚îÇ   ‚îú‚îÄ‚îÄ SUPERSET-ARCHITECTURE.md  # Arquitetura do Superset
‚îÇ   ‚îú‚îÄ‚îÄ SUPERSET-EXAMPLES.md      # Exemplos de uso
‚îÇ   ‚îî‚îÄ‚îÄ reports/                  # Relat√≥rios de testes e valida√ß√µes
‚îÇ       ‚îú‚îÄ‚îÄ RELATORIO-FINAL-TESTES-API-SUPERSET.md
‚îÇ       ‚îú‚îÄ‚îÄ RELATORIO-TESTES-API-SUPERSET.md
‚îÇ       ‚îú‚îÄ‚îÄ README-TESTES-API-SUPERSET.md
‚îÇ       ‚îî‚îÄ‚îÄ VALIDACAO_SUPERSET_API.md
‚îÇ
‚îú‚îÄ‚îÄ üìÇ examples/                   # Exemplos pr√°ticos
‚îÇ   ‚îú‚îÄ‚îÄ access_examples.py        # Scripts de acesso
‚îÇ   ‚îú‚îÄ‚îÄ automation/               # Automa√ß√£o de tarefas
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ exemplo_automacao_superset.py  # Classe SupersetAutomation
‚îÇ   ‚îú‚îÄ‚îÄ dags/                     # DAGs Airflow exemplo
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ etl_sales_pipeline.py
‚îÇ   ‚îú‚îÄ‚îÄ jobs/                     # Jobs Spark exemplo
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ process_sales.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ aggregate_sales.py
‚îÇ   ‚îú‚îÄ‚îÄ queries/                  # Queries SQL exemplo
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trino_examples.sql
‚îÇ   ‚îú‚îÄ‚îÄ notebooks/                # Jupyter notebooks
‚îÇ   ‚îî‚îÄ‚îÄ data/                     # Dados de exemplo
‚îÇ
‚îú‚îÄ‚îÄ üìÇ scripts/                    # Scripts de automa√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ README.md                 # Documenta√ß√£o dos scripts
‚îÇ   ‚îú‚îÄ‚îÄ setup_stack.py            # Setup completo da stack
‚îÇ   ‚îú‚îÄ‚îÄ validate_stack.py         # Valida√ß√£o de servi√ßos individuais
‚îÇ   ‚îú‚îÄ‚îÄ validate_all_services.py  # ‚≠ê Valida√ß√£o unificada de todos os servi√ßos
‚îÇ   ‚îú‚îÄ‚îÄ configure_minio.py        # Configura√ß√£o MinIO
‚îÇ   ‚îú‚îÄ‚îÄ configure_superset.py     # Configura√ß√£o Superset
‚îÇ   ‚îú‚îÄ‚îÄ configure_trino.py        # Configura√ß√£o Trino
‚îÇ   ‚îú‚îÄ‚îÄ 02_criar_datasets_virtuais_completo.py  # Cria√ß√£o de datasets
‚îÇ   ‚îî‚îÄ‚îÄ shell/                    # Scripts shell
‚îÇ       ‚îú‚îÄ‚îÄ setup.sh              # Setup inicial do ambiente
‚îÇ       ‚îú‚îÄ‚îÄ check-storage.sh      # Verifica√ß√£o de storage
‚îÇ       ‚îú‚îÄ‚îÄ validate-superset.sh  # Valida√ß√£o Superset
‚îÇ       ‚îî‚îÄ‚îÄ validate-superset-drivers.sh  # Valida√ß√£o drivers
‚îÇ
‚îú‚îÄ‚îÄ üìÇ tests/                      # Testes automatizados
‚îÇ   ‚îî‚îÄ‚îÄ superset/                 # Testes API Apache Superset
‚îÇ       ‚îú‚îÄ‚îÄ test_superset_api_complete.py      # Suite completa de testes
‚îÇ       ‚îú‚îÄ‚îÄ test_superset_crud_operations.py   # CRUD databases/datasets
‚îÇ       ‚îú‚îÄ‚îÄ test_superset_sql_queries.py       # Execu√ß√£o SQL via API
‚îÇ       ‚îú‚îÄ‚îÄ test_api_login_final.py
‚îÇ       ‚îú‚îÄ‚îÄ test_csrf_cookies.py
‚îÇ       ‚îú‚îÄ‚îÄ test_csrf_debug.py
‚îÇ       ‚îú‚îÄ‚îÄ test_form_csrf.py
‚îÇ       ‚îú‚îÄ‚îÄ test_session_csrf.py
‚îÇ       ‚îú‚îÄ‚îÄ test_superset_api.py
‚îÇ       ‚îú‚îÄ‚îÄ test_superset_complete.py
‚îÇ       ‚îú‚îÄ‚îÄ test_superset_simple.py
‚îÇ       ‚îî‚îÄ‚îÄ test_web_login.py
‚îÇ
‚îî‚îÄ‚îÄ üìÇ sql/                        # Views SQL anal√≠ticas
    ‚îú‚îÄ‚îÄ 01_vw_consistencia_alocacao.sql
    ‚îú‚îÄ‚îÄ 02_vw_horario_trabalho.sql
    ‚îú‚îÄ‚îÄ 03_vw_produtividade_horaria.sql
    ‚îú‚îÄ‚îÄ 04_vw_sensibilidade_preco.sql
    ‚îú‚îÄ‚îÄ 05_vw_vpn_projetos.sql
    ‚îú‚îÄ‚îÄ 06_vw_competitividade_salarial.sql
    ‚îú‚îÄ‚îÄ 07_vw_kpis_executivo.sql
    ‚îú‚îÄ‚îÄ 08_vw_capacidade_detalhada.sql
    ‚îú‚îÄ‚îÄ 09_vw_performance_projetos.sql
    ‚îú‚îÄ‚îÄ 10_vw_custos_rentabilidade.sql
    ‚îú‚îÄ‚îÄ 11_vw_qualidade_bugs.sql
    ‚îú‚îÄ‚îÄ 12_vw_sazonalidade_utilizacao.sql
    ‚îú‚îÄ‚îÄ 13_vw_ponto_equilibrio.sql
    ‚îî‚îÄ‚îÄ 14_vw_concentracao_hhi.sql
```

### üíæ Dados Persistidos (Disco Externo)

```
/media/marcelo/dados1/bigdata-docker/
‚îú‚îÄ‚îÄ postgres/      # üóÑÔ∏è  Metadados (Airflow, Superset, Hive)
‚îú‚îÄ‚îÄ minio/         # üì¶ Data Lake (arquivos principais)
‚îú‚îÄ‚îÄ airflow/       # üîÑ DAGs, logs, plugins
‚îú‚îÄ‚îÄ hive/          # üìö Warehouse do Hive
‚îú‚îÄ‚îÄ spark/         # ‚ö° Jobs, checkpoints, event logs
‚îú‚îÄ‚îÄ trino/         # üîç Cache de queries
‚îî‚îÄ‚îÄ superset/      # üìä Dashboards e configura√ß√µes
```

## üìÇ Organiza√ß√£o de Arquivos

### üß™ Testes (`tests/`)

Todos os testes automatizados est√£o organizados em `tests/superset/`:
- **test_superset_api_complete.py**: Suite completa com 85.7% de sucesso (12/14 testes)
- **test_superset_crud_operations.py**: Testes CRUD de databases, datasets, charts e dashboards
- **test_superset_sql_queries.py**: Testes de execu√ß√£o SQL via API (100% sucesso)
- Outros testes de autentica√ß√£o e CSRF

### üîß Scripts (`scripts/`)

Scripts de automa√ß√£o e configura√ß√£o:

**Python**:
- **validate_all_services.py**: ‚≠ê Valida√ß√£o unificada de todos os 8 servi√ßos da stack
- **validate_stack.py**: Valida√ß√£o individual de cada servi√ßo
- **setup_stack.py**: Setup automatizado completo
- **configure_*.py**: Configura√ß√£o de MinIO, Superset e Trino
- **02_criar_datasets_virtuais_completo.py**: Cria√ß√£o autom√°tica de datasets

**Shell** (`scripts/shell/`):
- **setup.sh**: Setup inicial do ambiente e estrutura de diret√≥rios
- **validate-superset.sh**: Valida√ß√£o espec√≠fica do Apache Superset
- **validate-superset-drivers.sh**: Verifica√ß√£o de drivers JDBC/Python
- **check-storage.sh**: Verifica√ß√£o de persist√™ncia e espa√ßo em disco

### üìö Exemplos (`examples/`)

Exemplos pr√°ticos e reutiliz√°veis:

**Automa√ß√£o** (`examples/automation/`):
- **exemplo_automacao_superset.py**: Classe `SupersetAutomation` com m√©todos para:
  - `create_database()`: Criar conex√µes de banco de dados
  - `create_virtual_dataset()`: Criar datasets virtuais (SQL)
  - `create_chart()`: Criar gr√°ficos
  - `create_dashboard()`: Criar dashboards
  - `execute_sql()`: Executar queries SQL via API

**DAGs Airflow** (`examples/dags/`):
- Pipeline ETL end-to-end com Spark

**Jobs Spark** (`examples/jobs/`):
- Processamento PySpark (Bronze ‚Üí Silver ‚Üí Gold)

### üìñ Documenta√ß√£o (`docs/`)

Documenta√ß√£o completa:
- Guias de uso de cada componente (01-09)
- **SUPERSET-v5-GUIA.md**: Guia do Apache Superset v5
- **reports/**: Relat√≥rios de testes e valida√ß√µes da API

### üóÑÔ∏è SQL (`sql/`)

Views anal√≠ticas prontas para uso:
- KPIs executivos, an√°lise de custos, produtividade, etc.
- 14 views SQL para an√°lises de neg√≥cio

## üõ†Ô∏è Comandos √öteis

## üõ†Ô∏è Comandos √öteis

### ÔøΩ Valida√ß√£o da Stack

```bash
# Validar todos os servi√ßos da stack (recomendado)
source .venv/bin/activate
python3 scripts/validate_all_services.py

# Sa√≠da esperada:
# ================================
# VALIDA√á√ÉO COMPLETA DA STACK MINI-BIGDATA
# ================================
# 
# Servi√ßos Validados: 8
# Servi√ßos OK: 8
# Taxa de Sucesso: 100.0%
# 
# ‚úì PostgreSQL - Databases: airflow, superset, metastore
# ‚úì Redis - Cache operacional
# ‚úì MinIO - Object storage operacional
# ‚úì Hive Metastore - Cat√°logo de metadados
# ‚úì Spark - Master e Workers operacionais
# ‚úì Trino - Query engine operacional
# ‚úì Airflow - Scheduler e Webserver ativos
# ‚úì Superset - BI Platform com 2 databases configurados

# Validar servi√ßos individuais
python3 scripts/validate_stack.py

# Validar apenas Superset
./scripts/shell/validate-superset.sh

# Validar drivers do Superset
./scripts/shell/validate-superset-drivers.sh

# Verificar storage
./scripts/shell/check-storage.sh
```

### üß™ Executar Testes da API Superset

```bash
# Ativar ambiente virtual Python
source .venv/bin/activate

# Instalar depend√™ncias (primeira vez)
pip install -r requirements.txt

# Suite completa de testes
python3 tests/superset/test_superset_api_complete.py

# Testes de CRUD (Databases, Datasets, Charts, Dashboards)
python3 tests/superset/test_superset_crud_operations.py

# Testes de execu√ß√£o SQL via API
python3 tests/superset/test_superset_sql_queries.py

# Exemplo de automa√ß√£o (classe reutiliz√°vel)
python3 examples/automation/exemplo_automacao_superset.py
```

### ÔøΩüöÄ Gerenciamento do Ambiente

```bash
# Iniciar todos os servi√ßos
docker-compose up -d

# Parar todos os servi√ßos (mant√©m dados)
docker-compose down

# Parar e remover volumes Docker (‚ö†Ô∏è dados em /media/marcelo/dados1/ s√£o mantidos)
docker-compose down -v

# Reiniciar ambiente completo
docker-compose restart

# Reiniciar servi√ßo espec√≠fico
docker-compose restart airflow-webserver
docker-compose restart spark-master
docker-compose restart trino

# Ver status dos containers
docker-compose ps

# Ver estat√≠sticas de recursos
docker stats
```

### üìã Logs e Debugging

```bash
# Ver logs de todos os servi√ßos
docker-compose logs -f

# Ver logs de um servi√ßo espec√≠fico
docker-compose logs -f trino
docker-compose logs -f spark-master
docker-compose logs -f airflow-scheduler

# Ver √∫ltimas 100 linhas
docker-compose logs --tail=100 airflow-webserver

# Acessar shell do container
docker-compose exec airflow-webserver bash
docker-compose exec spark-master bash
docker-compose exec trino /bin/bash
```

### ‚öôÔ∏è Escalabilidade

```bash
# Escalar workers do Airflow
docker-compose up -d --scale airflow-worker=3

# Escalar workers do Spark
docker-compose up -d --scale spark-worker=2

# Verificar workers ativos
docker-compose ps | grep worker
```

### üíæ Backup e Manuten√ß√£o

```bash
# Backup completo dos dados
tar -czf backup-bigdata-$(date +%Y%m%d).tar.gz \
  /media/marcelo/dados1/bigdata-docker/

# Backup espec√≠fico (apenas MinIO)
tar -czf backup-minio-$(date +%Y%m%d).tar.gz \
  /media/marcelo/dados1/bigdata-docker/minio/

# Verificar espa√ßo em disco
du -sh /media/marcelo/dados1/bigdata-docker/*/

# Limpar logs antigos do Airflow
docker-compose exec airflow-scheduler \
  airflow db clean --clean-before-timestamp "$(date -d '30 days ago' '+%Y-%m-%d')" -y

# Limpar cache do Docker
docker system prune -a --volumes
```

### üßπ Limpeza e Reset

```bash
# Reset completo (‚ö†Ô∏è CUIDADO: remove TODOS os dados)
docker-compose down -v
sudo rm -rf /media/marcelo/dados1/bigdata-docker/
./setup.sh
docker-compose up -d

# Rebuild de um servi√ßo espec√≠fico
docker-compose build --no-cache hive-metastore
docker-compose up -d hive-metastore
```

## üîß Customiza√ß√£o e Configura√ß√£o

## üîß Customiza√ß√£o e Configura√ß√£o

### üîê Alterar Credenciais Padr√£o

Edite as vari√°veis no arquivo `.env` ou diretamente no `docker-compose.yml`:

```bash
# MinIO
MINIO_ROOT_USER=seu_usuario
MINIO_ROOT_PASSWORD=SuaSenhaSegura123!

# PostgreSQL
POSTGRES_PASSWORD=postgres_senha_forte

# Airflow
AIRFLOW_WWW_USER_USERNAME=admin
AIRFLOW_WWW_USER_PASSWORD=admin_senha_forte
```

Ap√≥s alterar, reinicie os servi√ßos:

```bash
docker-compose down
docker-compose up -d
```

### üìä Adicionar Cat√°logos no Trino

Crie novos arquivos em `config/trino/catalog/`:

**MySQL Catalog** (`config/trino/catalog/mysql.properties`):
```properties
connector.name=mysql
connection-url=jdbc:mysql://mysql-host:3306
connection-user=root
connection-password=senha
```

**PostgreSQL Catalog** (`config/trino/catalog/postgresql.properties`):
```properties
connector.name=postgresql
connection-url=jdbc:postgresql://postgres-host:5432/database
connection-user=postgres
connection-password=senha
```

**Iceberg Catalog** (`config/trino/catalog/iceberg.properties`):
```properties
connector.name=iceberg
hive.metastore.uri=thrift://hive-metastore:9083
iceberg.catalog.type=hive_metastore
```

Reinicie o Trino ap√≥s adicionar cat√°logos:

```bash
docker-compose restart trino
```

### ‚ö° Ajustar Recursos do Spark

Edite `config/spark/spark-defaults.conf`:

```properties
# Mem√≥ria do driver
spark.driver.memory              2g

# Mem√≥ria do executor
spark.executor.memory            4g
spark.executor.cores             2

# N√∫mero de executores
spark.executor.instances         2

# Configura√ß√µes de shuffle
spark.sql.shuffle.partitions     200
```

### ÔøΩ Configurar Paralelismo do Airflow

Edite no `docker-compose.yml`:

```yaml
environment:
  AIRFLOW__CORE__PARALLELISM: 32
  AIRFLOW__CORE__DAG_CONCURRENCY: 16
  AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: 16
```

## üìä Formatos de Dados Recomendados

### üèóÔ∏è Arquitetura de Data Lake (Medallion)

| Camada | Formato | Compress√£o | Uso | Exemplo |
|--------|---------|------------|-----|---------|
| **ü•â Bronze** | JSON, CSV, Avro | gzip, snappy | Dados brutos | `s3a://bronze/raw/` |
| **ü•à Silver** | Parquet | snappy | Dados limpos | `s3a://silver/processed/` |
| **ü•á Gold** | Parquet, Delta, Iceberg | zstd | Analytics-ready | `s3a://gold/analytics/` |

### üí° Recomenda√ß√µes

- **Bronze**: Mantenha formato original, adicione apenas metadados (ingest_date, source, etc)
- **Silver**: Converta para Parquet, aplique schema, particione por data
- **Gold**: Use Delta/Iceberg para ACID e time-travel, agregue dados por caso de uso

## üîç Monitoramento e Observabilidade

## üîç Monitoramento e Observabilidade

### üìä UIs de Monitoramento

| Interface | URL | Descri√ß√£o |
|-----------|-----|-----------|
| **Airflow** | http://localhost:8080 | Status DAGs, task logs, conex√µes |
| **Spark Master** | http://localhost:8081 | Jobs, stages, executors |
| **Trino** | http://localhost:8085 | Query monitoring, cluster status |
| **MinIO Console** | http://localhost:9001 | Storage metrics, buckets, bandwidth |
| **Superset** | http://localhost:8088 | Dashboard usage, query logs |

### üìà M√©tricas Importantes

**Airflow**:
- DAG run duration
- Task success/failure rate
- Scheduler heartbeat

**Spark**:
- Job duration
- Stages e tasks completed
- Memory e CPU usage

**Trino**:
- Query execution time
- Data read/written
- Active queries

**MinIO**:
- Storage utilizado
- Bandwidth (upload/download)
- Request rate

### üîî Alertas (Opcional)

Para implementar alertas, considere adicionar:
- **Prometheus** + **Grafana** para m√©tricas
- **Alertmanager** para notifica√ß√µes
- Configura√ß√£o de email no Airflow para falhas de DAG

## üêõ Troubleshooting

### ‚ùå Problemas Comuns

<details>
<summary><b>Servi√ßos n√£o sobem / Container em estado "unhealthy"</b></summary>

```bash
# Verificar logs do servi√ßo espec√≠fico
docker-compose logs trino
docker-compose logs airflow-webserver

# Verificar recursos dispon√≠veis
docker stats

# Recriar containers
docker-compose down
docker-compose up -d --force-recreate
```
</details>

<details>
<summary><b>Airflow n√£o conecta ao Spark</b></summary>

1. Verificar conex√£o no Airflow Admin ‚Üí Connections
2. Conn Id: `spark_default`
3. Host: `spark://spark-master:7077`
4. Reiniciar scheduler: `docker-compose restart airflow-scheduler`
</details>

<details>
<summary><b>Trino n√£o encontra tabelas</b></summary>

```bash
# Verificar se Hive Metastore est√° rodando
docker-compose ps hive-metastore

# Conectar ao Trino e verificar cat√°logos
docker-compose exec trino trino
> SHOW CATALOGS;
> SHOW SCHEMAS FROM hive;
> SHOW TABLES FROM hive.default;

# Verificar permiss√µes no MinIO
# Acesse http://localhost:9001 e verifique buckets
```
</details>

<details>
<summary><b>Falta de espa√ßo em disco</b></summary>

```bash
# Verificar uso de disco
df -h /media/marcelo/dados1/

# Ver tamanho por componente
du -sh /media/marcelo/dados1/bigdata-docker/*/

# Limpar logs antigos
docker-compose exec airflow-scheduler airflow db clean --clean-before-timestamp "$(date -d '30 days ago' '+%Y-%m-%d')" -y

# Limpar cache Docker
docker system prune -a
```
</details>

<details>
<summary><b>Porta j√° em uso</b></summary>

```bash
# Verificar processo usando porta
sudo lsof -i :8080
sudo lsof -i :9000

# Matar processo (exemplo)
sudo kill -9 <PID>

# Ou alterar portas no docker-compose.yml
ports:
  - "8081:8080"  # Muda porta externa
```
</details>

### üìö Documenta√ß√£o de Troubleshooting

Para problemas mais complexos, consulte:
- **[TROUBLESHOOTING.md](TROUBLESHOOTING.md)** - Guia completo de solu√ß√£o de problemas
- **[STORAGE.md](STORAGE.md)** - Problemas relacionados a persist√™ncia
- **[SUPERSET-DRIVERS.md](SUPERSET-DRIVERS.md)** - Configura√ß√£o de drivers do Superset
- Issues do projeto no GitHub

## üöÄ Roadmap e Melhorias Futuras

## üöÄ Roadmap e Melhorias Futuras

### üéØ Pr√≥ximas Implementa√ß√µes

- [ ] **Apache Iceberg / Delta Lake** - Table format para ACID transactions
- [ ] **Apache Kafka** - Streaming de dados em tempo real
- [ ] **Jupyter Lab** - Notebooks para an√°lises interativas
- [ ] **dbt (Data Build Tool)** - Transforma√ß√µes SQL como c√≥digo
- [ ] **Great Expectations** - Data quality e testes
- [ ] **Apache Atlas** - Data governance e lineage
- [ ] **Prometheus + Grafana** - Monitoring avan√ßado
- [ ] **Apache Ranger** - Security e controle de acesso
- [ ] **MLflow** - Machine Learning lifecycle
- [ ] **Dagster** - Alternativa moderna ao Airflow

### ÔøΩ Ideias de Contribui√ß√£o

Contribui√ß√µes s√£o bem-vindas! Veja como voc√™ pode ajudar:

1. üêõ Reportar bugs via [Issues](https://github.com/marcelolimagomes/mini-bigdata/issues)
2. üìñ Melhorar documenta√ß√£o
3. ‚ú® Adicionar novos exemplos pr√°ticos
4. üîß Propor otimiza√ß√µes de configura√ß√£o
5. üé® Criar dashboards de exemplo no Superset
6. üìä Adicionar datasets de exemplo

## üìö Recursos de Aprendizado

### üìñ Documenta√ß√£o Oficial

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [Trino Documentation](https://trino.io/docs/current/)
- [MinIO Documentation](https://min.io/docs/minio/linux/index.html)
- [Apache Superset Documentation](https://superset.apache.org/docs/intro)
- [Hive Metastore](https://cwiki.apache.org/confluence/display/Hive/AdminManual+Metastore+Administration)

### üéì Tutoriais e Cursos

- [Fundamentals of Data Engineering (Joe Reis)](https://www.oreilly.com/library/view/fundamentals-of-data/9781098108298/)
- [Data Engineering Cookbook](https://github.com/andkret/Cookbook)
- [Awesome Data Engineering](https://github.com/igorbarinov/awesome-data-engineering)

### üåê Comunidades

- [Data Engineering Brasil](https://www.linkedin.com/groups/12345678/)
- [Stack Overflow - Tags: airflow, spark, trino](https://stackoverflow.com/)
- [Reddit r/dataengineering](https://www.reddit.com/r/dataengineering/)

## ü§ù Contribuindo

Contribui√ß√µes s√£o muito bem-vindas! Para contribuir:

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Add: nova feature incr√≠vel'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

### üìã Guidelines

- Mantenha o c√≥digo limpo e documentado
- Adicione exemplos pr√°ticos quando poss√≠vel
- Atualize a documenta√ß√£o relevante
- Teste suas mudan√ßas antes de submeter PR

## üìÑ Licen√ßa

Este projeto est√° licenciado sob a **MIT License** - veja o arquivo [LICENSE](LICENSE) para detalhes.

Voc√™ √© livre para:
- ‚úÖ Usar comercialmente
- ‚úÖ Modificar
- ‚úÖ Distribuir
- ‚úÖ Uso privado

## üë§ Autor

**Marcelo Lima Gomes**

- üåê GitHub: [@marcelolimagomes](https://github.com/marcelolimagomes)
- üìß Email: marcelolimagomes@gmail.com
- üíº LinkedIn: [Marcelo Lima Gomes](https://www.linkedin.com/in/marcelolimagomes/)

## ‚≠ê Agradecimentos

Este projeto foi inspirado por diversas fontes da comunidade open-source:

- Apache Software Foundation
- Trino Community
- MinIO Project
- Docker Community

## üôè Apoie o Projeto

Se este projeto foi √∫til para voc√™:

- ‚≠ê D√™ uma estrela no GitHub
- üîÄ Fa√ßa um fork
- üì¢ Compartilhe com sua rede
- üí¨ D√™ feedback via Issues
- ü§ù Contribua com melhorias

## üìû Suporte

- üêõ **Bugs**: Abra uma [issue](https://github.com/marcelolimagomes/mini-bigdata/issues)
- üí¨ **D√∫vidas**: Use as [Discussions](https://github.com/marcelolimagomes/mini-bigdata/discussions)
- üìß **Contato Direto**: marcelolimagomes@gmail.com

---

<div align="center">

**Desenvolvido com ‚ù§Ô∏è para a comunidade de Big Data e DevOps**

[![GitHub](https://img.shields.io/badge/GitHub-marcelolimagomes-181717?style=for-the-badge&logo=github)](https://github.com/marcelolimagomes)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge)](https://opensource.org/licenses/MIT)

**[‚¨Ü Voltar ao topo](#-mini-big-data-platform)**

</div>
