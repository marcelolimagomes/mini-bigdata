# ‚ö° Instala√ß√£o R√°pida - Mini Big Data Platform

## üéØ Pr√©-requisitos

- [x] Docker e Docker Compose instalados
- [x] M√≠nimo 8GB RAM
- [x] **20GB+ espa√ßo livre em `/media/marcelo/dados1/`**
- [x] Portas livres: 5432, 7077, 8080-8088, 9000-9001, 9083

## üöÄ Instala√ß√£o em 3 Passos

### 1Ô∏è‚É£ Preparar Armazenamento
```bash
./check-storage.sh
```
**O que faz:**
- ‚úÖ Verifica espa√ßo em disco
- ‚úÖ Cria estrutura em `/media/marcelo/dados1/bigdata-docker/`
- ‚úÖ Configura permiss√µes
- ‚úÖ Oferece backup de dados existentes

### 2Ô∏è‚É£ Configurar Ambiente
```bash
./setup.sh
```
**O que faz:**
- ‚úÖ Cria diret√≥rios de configura√ß√£o
- ‚úÖ Gera arquivos de config (Spark, Trino, Hive, etc.)
- ‚úÖ Valida Docker

### 3Ô∏è‚É£ Iniciar Servi√ßos
```bash
docker-compose up -d
```
**O que faz:**
- ‚úÖ Sobe todos os 11 servi√ßos
- ‚úÖ Cria buckets no MinIO
- ‚úÖ Inicializa bancos de dados
- ‚úÖ Configura usu√°rios

## üìä Acompanhar Inicializa√ß√£o

```bash
# Ver logs em tempo real
docker-compose logs -f

# Verificar status
docker-compose ps

# Ver apenas servi√ßos saud√°veis
docker-compose ps | grep healthy
```

## üåê Acessar Interfaces

Aguarde ~2-3 minutos para todos os servi√ßos subirem, depois acesse:

| Servi√ßo | URL | Credenciais |
|---------|-----|-------------|
| **MinIO** | http://localhost:9001 | minioadmin / minioadmin123 |
| **Airflow** | http://localhost:8080 | airflow / airflow |
| **Superset** | http://localhost:8088 | admin / admin |
| **Trino** | http://localhost:8085 | trino (sem senha) |
| **Spark Master** | http://localhost:8081 | - |

## ‚úÖ Validar Instala√ß√£o

### Teste 1: Verificar todos os servi√ßos
```bash
docker-compose ps
```
**Esperado:** Todos com status `Up` e alguns com `healthy`

### Teste 2: Verificar dados persistidos
```bash
du -sh /media/marcelo/dados1/bigdata-docker/*/
```
**Esperado:** Ver diret√≥rios com alguns MBs

### Teste 3: Acessar MinIO
1. Abra: http://localhost:9001
2. Login: minioadmin / minioadmin123
3. Veja buckets: bronze, silver, gold

### Teste 4: Executar pipeline de exemplo
1. Acesse Airflow: http://localhost:8080
2. Login: airflow / airflow
3. Ative DAG: `etl_sales_pipeline`
4. Clique em "Trigger DAG"
5. Aguarde conclus√£o (~2-5 minutos)

### Teste 5: Consultar dados no Trino
```bash
docker-compose exec trino trino --execute "SHOW CATALOGS;"
```
**Esperado:** Ver `hive`, `memory`, `system`

## üéì Primeiros Passos

### 1. Explorar MinIO
```bash
# Via browser
open http://localhost:9001

# Ver buckets criados
docker-compose exec minio-client mc ls myminio/
```

### 2. Executar primeira DAG
- Acesse Airflow
- Ative e execute `etl_sales_pipeline`
- Acompanhe execu√ß√£o no Graph View

### 3. Consultar dados processados
```sql
-- No Trino (http://localhost:8085)
SELECT * FROM hive.sales.sales_silver LIMIT 10;
```

### 4. Criar primeiro dashboard
- Acesse Superset
- Add Database: `trino://trino@trino:8080/hive/sales`
- SQL Lab > Execute queries
- Charts > Create chart

## üìö Documenta√ß√£o

| Arquivo | Conte√∫do |
|---------|----------|
| **README.md** | Documenta√ß√£o completa |
| **QUICKSTART.md** | Guia r√°pido de uso |
| **STORAGE.md** | Detalhes de armazenamento |
| **CHANGELOG-STORAGE.md** | Mudan√ßas de persist√™ncia |
| **TROUBLESHOOTING.md** | Solu√ß√£o de problemas |

## üÜò Problemas?

### Servi√ßo n√£o sobe
```bash
docker-compose logs <nome-servico>
```

### Porta em uso
```bash
sudo netstat -tulpn | grep <porta>
docker-compose down
# Matar processo ou mudar porta no .env
```

### Sem espa√ßo
```bash
df -h /media/marcelo/dados1/
# Limpar ou usar outro disco (veja STORAGE.md)
```

### Permiss√µes
```bash
sudo chown -R $USER:$USER /media/marcelo/dados1/bigdata-docker/
sudo chown -R 50000:0 /media/marcelo/dados1/bigdata-docker/airflow/
```

## üõë Parar/Remover

```bash
# Parar servi√ßos (mant√©m dados)
docker-compose down

# Parar e remover volumes Docker (dados em /media persistem!)
docker-compose down -v

# Limpar TUDO (incluindo dados)
docker-compose down -v
sudo rm -rf /media/marcelo/dados1/bigdata-docker/
```

## üíæ Backup

```bash
# Backup completo
tar -czf bigdata-backup-$(date +%Y%m%d).tar.gz \
  /media/marcelo/dados1/bigdata-docker/

# Apenas dados (MinIO)
tar -czf minio-backup-$(date +%Y%m%d).tar.gz \
  /media/marcelo/dados1/bigdata-docker/minio/
```

## üìà Monitoramento

```bash
# Uso de recursos
docker stats

# Espa√ßo em disco
watch -n 5 'du -sh /media/marcelo/dados1/bigdata-docker/*/'

# Status dos servi√ßos
watch -n 10 'docker-compose ps'
```

## üéØ Checklist de Instala√ß√£o

- [ ] Executei `./check-storage.sh` com sucesso
- [ ] Executei `./setup.sh` com sucesso
- [ ] Executei `docker-compose up -d`
- [ ] Aguardei 2-3 minutos
- [ ] Todos os servi√ßos est√£o `Up`
- [ ] Consegui acessar MinIO (9001)
- [ ] Consegui acessar Airflow (8080)
- [ ] Consegui acessar Superset (8088)
- [ ] Consegui acessar Trino (8085)
- [ ] Executei DAG de exemplo com sucesso
- [ ] Dados aparecem em `/media/marcelo/dados1/bigdata-docker/`
- [ ] Li a documenta√ß√£o (README.md)

## ‚ú® Pr√≥ximos Passos

1. **Explorar exemplos**: Veja `examples/` para DAGs, Jobs e Queries
2. **Criar seu pipeline**: Copie e modifique DAG de exemplo
3. **Upload de dados**: Use MinIO para fazer upload de seus CSVs
4. **Dashboards**: Crie visualiza√ß√µes no Superset
5. **Escalar**: Adicione mais workers Spark conforme necess√°rio

---

**Instala√ß√£o completa em ~5 minutos!** üöÄ

Para d√∫vidas, consulte **TROUBLESHOOTING.md** ou abra uma issue.
